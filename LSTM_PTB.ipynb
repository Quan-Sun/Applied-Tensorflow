{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>Based on<a href=\"https://github.com/jiqizhixin/ML-Tutorial-Experiment/blob/master/Experiments/LSTM_PTB.ipynb\"> HoratioJSY</a></strong><p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/quansun/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import collections\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_words(filename):\n",
    "    with tf.gfile.GFile(filename,'r') as f:\n",
    "        return f.read().replace('\\n','<eos>').split()\n",
    "    \n",
    "def _build_vocab(filename):\n",
    "    data = _read_words(filename)\n",
    "    counter = collections.Counter(data)\n",
    "    count_pairs = sorted(counter.items(),key=lambda x:(-x[1],x[0]))\n",
    "    \n",
    "    words,_ = list(zip(*count_pairs))\n",
    "    word_to_id = dict(zip(words,range(len(words))))\n",
    "    \n",
    "    return word_to_id\n",
    "\n",
    "def _file_to_word_ids(filename,word_to_id):\n",
    "    data = _read_words(filename)\n",
    "    return [word_to_id[word] for word in data if word in word_to_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ptb_raw_data(data_path):\n",
    "    train_path = os.path.join(data_path,'ptb.train.txt')\n",
    "    valid_path = os.path.join(data_path,'ptb.valid.txt')\n",
    "    test_path = os.path.join(data_path,'ptb.test.txt')\n",
    "    \n",
    "    word_to_id = _build_vocab(train_path)\n",
    "    train_data = _file_to_word_ids(train_path,word_to_id)\n",
    "    valid_data = _file_to_word_ids(valid_path,word_to_id)\n",
    "    test_data = _file_to_word_ids(test_path,word_to_id)\n",
    "    word_len=len(word_to_id)\n",
    "    return train_data,valid_data,test_data,word_len\n",
    "\n",
    "def ptb_producer(raw_data,batch_size,num_steps,name=None):\n",
    "    with tf.name_scope(name,'PTBProducer',[raw_data,batch_size,num_steps]):\n",
    "        raw_data=tf.convert_to_tensor(raw_data,name='raw_data',dtype=tf.int32)\n",
    "        data_len=tf.size(raw_data)\n",
    "        batch_len=data_len//batch_size\n",
    "        data=tf.reshape(raw_data[0:batch_size*batch_len],\n",
    "                       [batch_size,batch_len])\n",
    "        epoch_size=(batch_len-1)//num_steps\n",
    "        assertion = tf.assert_positive(\n",
    "            epoch_size,\n",
    "            message='epoch_size==0,decrease batch_size or num_steps')\n",
    "        with tf.control_dependencies([assertion]):\n",
    "            epoch_size=tf.identity(epoch_size,name='epoch_size')\n",
    "        \n",
    "        i = tf.train.range_input_producer(epoch_size,shuffle=False).dequeue()\n",
    "        x = tf.strided_slice(data,[0,i*num_steps],\n",
    "                            [batch_size,(i+1)*num_steps])\n",
    "        x.set_shape([batch_size,num_steps])\n",
    "        y = tf.strided_slice(data,[0,i*num_steps+1],\n",
    "                            [batch_size,(i+1)*num_steps+1])\n",
    "        y.set_shape([batch_size,num_steps])\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './PTB'\n",
    "\n",
    "# the number of hidden unit and the layer of LSTM\n",
    "hidden_size = 200\n",
    "num_layers = 2\n",
    "#the size of vocabulary\n",
    "vocab_size = 10000\n",
    "\n",
    "learning_rate = 1.0\n",
    "train_batch_size = 16\n",
    "# the length of truncating\n",
    "train_num_step = 32\n",
    "\n",
    "eval_batch_size = 1\n",
    "eval_num_step = 1\n",
    "num_epoch = 3\n",
    "#dropout probability\n",
    "keep_prob = 0.5\n",
    "\n",
    "# the parameter of controling gradient explosion\n",
    "max_grad_norm = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTBModel(object):\n",
    "    def __init__(self, is_training, batch_size, num_steps):\n",
    "        # batch size and trucating length\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "\n",
    "        # input layer\n",
    "        self.input_data = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "        # output layer\n",
    "        self.targets = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "\n",
    "        # LSTM cell\n",
    "        lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size)\n",
    "        if is_training:\n",
    "            lstm_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob=keep_prob)\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * num_layers)\n",
    "\n",
    "        # initialize state to 0\n",
    "        self.initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "        # vectorize vocabulary ids\n",
    "        embedding = tf.get_variable('embedding', [vocab_size, hidden_size])\n",
    "        # vectorization\n",
    "        inputs = tf.nn.embedding_lookup(embedding, self.input_data)\n",
    "\n",
    "        # dropout only when training\n",
    "        if is_training: inputs = tf.nn.dropout(inputs, keep_prob)\n",
    "\n",
    "        # store output of lstm\n",
    "        outputs = []\n",
    "        # store LSTM state of different batch, initial 0\n",
    "        state = self.initial_state\n",
    "        with tf.variable_scope('RNN'):\n",
    "            for time_step in range(num_steps):\n",
    "                if time_step > 0: \n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "                # get input of current time step and state of last time step, then feed forward to LSTM\n",
    "                cell_output, state = cell(inputs[:, time_step, :], state)\n",
    "            \n",
    "                outputs.append(cell_output)\n",
    "\n",
    "        # reshape to [batch,hidden*num_step], then reshape to [batch*num_step, hidden]\n",
    "        output = tf.reshape(tf.concat(outputs, 1), [-1, hidden_size])\n",
    "\n",
    "        # feed the outputs of LSTM to fully connected layer to get results. The results each is tensor which shape is vocab_size\n",
    "        # Then go through softmax layer to get the probability of vocab at the next position\n",
    "        weight = tf.get_variable('weight', [hidden_size, vocab_size])\n",
    "        bias = tf.get_variable('bias', [vocab_size])\n",
    "        logits = tf.matmul(output, weight) + bias\n",
    "\n",
    "        # loss\n",
    "        loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(\n",
    "            [logits],  # prediction\n",
    "            [tf.reshape(self.targets, [-1])],  # ground truth，flatten [batch_size, num_steps]\n",
    "            [tf.ones([batch_size * num_steps], dtype=tf.float32)])  # weights of loss，1 means the same weights\n",
    "\n",
    "        # avg loss every batch\n",
    "        self.cost = tf.reduce_sum(loss) / batch_size\n",
    "        self.final_state = state\n",
    "\n",
    "        # backpropagation only when training\n",
    "        if not is_training: return\n",
    "        trainable_variable = tf.trainable_variables()\n",
    "\n",
    "        # control gradient explosion\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, trainable_variable), max_grad_norm)\n",
    "        \n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        # train step\n",
    "        self.train_op = optimizer.apply_gradients(zip(grads, trainable_variable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(session, model, data, train_op, output_log, epoch_size):\n",
    "    total_costs = 0.0\n",
    "    iters = 0\n",
    "    state = session.run(model.initial_state)\n",
    "\n",
    "    \n",
    "    for step in range(epoch_size):\n",
    "        x, y = session.run(data)\n",
    "        \n",
    "        cost, state, _ = session.run([model.cost, model.final_state, train_op],\n",
    "                                        {model.input_data: x, model.targets: y, model.initial_state: state})\n",
    "        \n",
    "        total_costs += cost\n",
    "        iters += model.num_steps\n",
    "\n",
    "        # log only when training\n",
    "        if output_log and step % 100 == 0:\n",
    "            print(\"After %d steps, perplexity is %.3f\" % (step, np.exp(total_costs / iters)))\n",
    "    return np.exp(total_costs / iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    train_data, valid_data, test_data, _ = ptb_raw_data(data_path)\n",
    "\n",
    "    # calculate the number of training one epoch\n",
    "    train_data_len = len(train_data)\n",
    "    train_batch_len = train_data_len // train_batch_size\n",
    "    train_epoch_size = (train_batch_len - 1) // train_num_step\n",
    "\n",
    "    valid_data_len = len(valid_data)\n",
    "    valid_batch_len = valid_data_len // eval_batch_size\n",
    "    valid_epoch_size = (valid_batch_len - 1) // eval_num_step\n",
    "\n",
    "    test_data_len = len(test_data)\n",
    "    test_batch_len = test_data_len // eval_batch_size\n",
    "    test_epoch_size = (test_batch_len - 1) // eval_num_step\n",
    "\n",
    "    initializer = tf.random_uniform_initializer(-0.05, 0.05)\n",
    "    with tf.variable_scope(\"language_model\", reuse=None, initializer=initializer):\n",
    "        train_model = PTBModel(True, train_batch_size, train_num_step)\n",
    "\n",
    "    with tf.variable_scope(\"language_model\", reuse=True, initializer=initializer):\n",
    "        eval_model = PTBModel(False, eval_batch_size, eval_num_step)\n",
    "\n",
    "    # training\n",
    "    with tf.Session() as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        train_queue = ptb_producer(train_data, train_model.batch_size, train_model.num_steps)\n",
    "        eval_queue = ptb_producer(valid_data, eval_model.batch_size, eval_model.num_steps)\n",
    "        test_queue = ptb_producer(test_data, eval_model.batch_size, eval_model.num_steps)\n",
    "\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=session, coord=coord)\n",
    "\n",
    "        for i in range(num_epoch):\n",
    "            print(\"In iteration: %d\" % (i + 1))\n",
    "            run_epoch(session, train_model, train_queue, train_model.train_op, True, train_epoch_size)\n",
    "\n",
    "            valid_perplexity = run_epoch(session, eval_model, eval_queue, tf.no_op(), False, valid_epoch_size)\n",
    "            print(\"Epoch: %d Validation Perplexity: %.3f\" % (i + 1, valid_perplexity))\n",
    "\n",
    "        test_perplexity = run_epoch(session, eval_model, test_queue, tf.no_op(), False, test_epoch_size)\n",
    "        print(\"Test Perplexity: %.3f\" % test_perplexity)\n",
    "\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In iteration: 1\n",
      "After 0 steps, perplexity is 10040.366\n",
      "After 100 steps, perplexity is 1415.210\n",
      "After 200 steps, perplexity is 1019.100\n",
      "After 300 steps, perplexity is 886.449\n",
      "After 400 steps, perplexity is 775.639\n",
      "After 500 steps, perplexity is 690.577\n",
      "After 600 steps, perplexity is 632.798\n",
      "After 700 steps, perplexity is 590.683\n",
      "After 800 steps, perplexity is 557.514\n",
      "After 900 steps, perplexity is 529.996\n",
      "After 1000 steps, perplexity is 505.546\n",
      "After 1100 steps, perplexity is 485.353\n",
      "After 1200 steps, perplexity is 467.655\n",
      "After 1300 steps, perplexity is 453.902\n",
      "After 1400 steps, perplexity is 441.095\n",
      "After 1500 steps, perplexity is 430.351\n",
      "After 1600 steps, perplexity is 416.922\n",
      "After 1700 steps, perplexity is 407.194\n",
      "After 1800 steps, perplexity is 399.936\n",
      "Epoch: 1 Validation Perplexity: 247.690\n",
      "In iteration: 2\n",
      "After 0 steps, perplexity is 434.120\n",
      "After 100 steps, perplexity is 288.719\n",
      "After 200 steps, perplexity is 269.692\n",
      "After 300 steps, perplexity is 276.311\n",
      "After 400 steps, perplexity is 270.414\n",
      "After 500 steps, perplexity is 260.722\n",
      "After 600 steps, perplexity is 256.543\n",
      "After 700 steps, perplexity is 253.593\n",
      "After 800 steps, perplexity is 251.992\n",
      "After 900 steps, perplexity is 250.383\n",
      "After 1000 steps, perplexity is 248.267\n",
      "After 1100 steps, perplexity is 246.687\n",
      "After 1200 steps, perplexity is 244.921\n",
      "After 1300 steps, perplexity is 244.696\n",
      "After 1400 steps, perplexity is 243.871\n",
      "After 1500 steps, perplexity is 243.516\n",
      "After 1600 steps, perplexity is 240.576\n",
      "After 1700 steps, perplexity is 239.584\n",
      "After 1800 steps, perplexity is 239.625\n",
      "Epoch: 2 Validation Perplexity: 197.504\n",
      "In iteration: 3\n",
      "After 0 steps, perplexity is 361.814\n",
      "After 100 steps, perplexity is 238.066\n",
      "After 200 steps, perplexity is 223.331\n",
      "After 300 steps, perplexity is 230.752\n",
      "After 400 steps, perplexity is 227.225\n",
      "After 500 steps, perplexity is 219.952\n",
      "After 600 steps, perplexity is 217.078\n",
      "After 700 steps, perplexity is 215.164\n",
      "After 800 steps, perplexity is 214.575\n",
      "After 900 steps, perplexity is 214.165\n",
      "After 1000 steps, perplexity is 213.112\n",
      "After 1100 steps, perplexity is 212.473\n",
      "After 1200 steps, perplexity is 211.422\n",
      "After 1300 steps, perplexity is 211.939\n",
      "After 1400 steps, perplexity is 211.816\n",
      "After 1500 steps, perplexity is 211.987\n",
      "After 1600 steps, perplexity is 209.826\n",
      "After 1700 steps, perplexity is 209.383\n",
      "After 1800 steps, perplexity is 209.983\n",
      "Epoch: 3 Validation Perplexity: 179.678\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
