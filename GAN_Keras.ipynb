{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/quansun/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Activation,Conv2D,MaxPooling2D\n",
    "from keras.layers import UpSampling2D,Flatten,Reshape,BatchNormalization\n",
    "from keras.optimizers import SGD\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import argparse\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(input_dim=100,output_dim=1024))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dense(128*7*7))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Reshape((7,7,128),input_shape=(128*7*7,)))\n",
    "    model.add(UpSampling2D(size=(2,2)))\n",
    "    model.add(Conv2D(64,(5,5),padding='same'))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(UpSampling2D(size=(2,2)))\n",
    "    model.add(Conv2D(1,(5,5),padding='same'))\n",
    "    model.add(Activation('tanh'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64,(5,5),padding='same',input_shape=(28,28,1)))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(128,(5,5)))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_given_discriminator(generator_model,discriminator_model):\n",
    "    model = Sequential()\n",
    "    model.add(generator_model)\n",
    "    d.trainable = False\n",
    "    model.add(discriminator_model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_images(generated_images):\n",
    "    num = generated_images.shape[0]\n",
    "    width = int(math.sqrt(num))\n",
    "    height = int(math.ceil(float(num)/width))\n",
    "    shape = generated_images.shape[1:3]\n",
    "    image = np.zeros((height*shape[0],width*shape[1]),\n",
    "                    dtype=generated_images.dtype)\n",
    "    for index,img in enumerate(generated_images):\n",
    "        i = int(index/width)\n",
    "        j = index % width\n",
    "        image[i*shape[0]:(i+1)*shape[0],j*shape[1]:(j+1)*shape[1]] = img[:,:,0]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(BATCH_SIZE):\n",
    "    (X_train,y_train),(X_test,y_test) = mnist.load_data()\n",
    "    X_train = (X_train.astype(np.float32)-127.5)/127.5\n",
    "    X_train = X_train[:,:,:,None]\n",
    "    X_test = X_test[:,:,:,None]\n",
    "    \n",
    "    d = discriminator()\n",
    "    g = generator()\n",
    "    d_on_g = generator_given_discriminator(g,d)\n",
    "    \n",
    "    d_optim = SGD(lr=0.001,momentum=0.9,nesterov=True)\n",
    "    g_optim = SGD(lr=0.001,momentum=0.9,nesterov=True)\n",
    "    \n",
    "    g.compile(loss='binary_crossentropy',optimizer='SGD')\n",
    "    d_on_g.compile(loss='binary_crossentropy',optimizer=g_optim)\n",
    "    \n",
    "    d.trainable=True\n",
    "    d.compile(loss='binary_crossentropy',optimizer=d_optim)\n",
    "    \n",
    "    for epoch in range(30):\n",
    "        print('Epoch is', epoch)\n",
    "        print('Number of batches', int(X_train.shape[0]/BATCH_SIZE))\n",
    "        \n",
    "        for index in range(int(X_train.shape[0]/BATCH_SIZE)):\n",
    "            noise = np.random.uniform(-1,1,size=(BATCH_SIZE,100))\n",
    "            image_batch=X_train[index*BATCH_SIZE:(index+1)*BATCH_SIZE]\n",
    "            generated_images=g.predict(noise,verbose=0)\n",
    "            if index%100==0:\n",
    "                image = combine_images(generated_images)\n",
    "                image = image*127.5+127.5\n",
    "                Image.fromarray(image.astype(np.uint8)).save('./GAN/'+str(epoch)+'_'+str(index)+'.jpg')\n",
    "                \n",
    "            X = np.concatenate((image_batch,generated_images))\n",
    "            \n",
    "            y = [1]*BATCH_SIZE + [0] * BATCH_SIZE\n",
    "            \n",
    "            d_loss = d.train_on_batch(X,y)\n",
    "            print('batch %d d_loss : %f' % (index, d_loss))\n",
    "            \n",
    "            noise = np.random.uniform(-1,1,(BATCH_SIZE,100))\n",
    "            \n",
    "            d.trainable = False\n",
    "            \n",
    "            g_loss = d_on_g.train_on_batch(noise,[1]*BATCH_SIZE)\n",
    "            \n",
    "            d.trainable = True\n",
    "            print('batch %d g_loss : %f' % (index,g_loss))\n",
    "            \n",
    "            if index % 100 == 9:\n",
    "                g.save_weights('generator', True)\n",
    "                d.save_weights('discriminator', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(BATCH_SIZE, nice= False ):\n",
    "    \n",
    "    g = generator()\n",
    "    g.compile(loss='binary_crossentropy', optimizer=\"SGD\")\n",
    "    g.load_weights('generator')\n",
    "    if nice:\n",
    "        d = discriminator()\n",
    "        d.compile(loss='binary_crossentropy', optimizer=\"SGD\")\n",
    "        d.load_weights('discriminator')\n",
    "        noise = np.random.uniform(-1, 1, (BATCH_SIZE*20, 100))\n",
    "        generated_images = g.predict(noise, verbose=1)\n",
    "        d_pret = d.predict(generated_images, verbose=1)\n",
    "        index = np.arange(0, BATCH_SIZE*20)\n",
    "        index.resize((BATCH_SIZE*20, 1))\n",
    "        pre_with_index = list(np.append(d_pret, index, axis=1))\n",
    "        pre_with_index.sort(key=lambda x: x[0], reverse=True)\n",
    "        nice_images = np.zeros((BATCH_SIZE,) + generated_images.shape[1:3], dtype=np.float32)\n",
    "        nice_images = nice_images[:, :, :, None]\n",
    "        for i in range(BATCH_SIZE):\n",
    "            idx = int(pre_with_index[i][1])\n",
    "            nice_images[i, :, :, 0] = generated_images[idx, :, :, 0]\n",
    "        image = combine_images(nice_images)\n",
    "    else:\n",
    "        noise = np.random.uniform(-1, 1, (BATCH_SIZE, 100))\n",
    "        generated_images = g.predict(noise, verbose=0)\n",
    "        image = combine_images(generated_images)\n",
    "    image = image*127.5+127.5\n",
    "    Image.fromarray(image.astype(np.uint8)).save(\n",
    "        \"./GAN/generated_image.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/quansun/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=100, units=1024)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch is 0\n",
      "Number of batches 454\n",
      "batch 0 d_loss : 0.667026\n",
      "batch 0 g_loss : 0.707065\n",
      "batch 1 d_loss : 0.640115\n",
      "batch 1 g_loss : 0.697564\n",
      "batch 2 d_loss : 0.613985\n",
      "batch 2 g_loss : 0.687107\n",
      "batch 3 d_loss : 0.582559\n",
      "batch 3 g_loss : 0.682326\n",
      "batch 4 d_loss : 0.560834\n",
      "batch 4 g_loss : 0.673343\n",
      "batch 5 d_loss : 0.544283\n",
      "batch 5 g_loss : 0.665048\n",
      "batch 6 d_loss : 0.521645\n",
      "batch 6 g_loss : 0.668004\n",
      "batch 7 d_loss : 0.493725\n",
      "batch 7 g_loss : 0.662798\n",
      "batch 8 d_loss : 0.475911\n",
      "batch 8 g_loss : 0.673772\n",
      "batch 9 d_loss : 0.479765\n",
      "batch 9 g_loss : 0.673942\n",
      "batch 10 d_loss : 0.455973\n",
      "batch 10 g_loss : 0.685020\n",
      "batch 11 d_loss : 0.430585\n",
      "batch 11 g_loss : 0.679927\n",
      "batch 12 d_loss : 0.418536\n",
      "batch 12 g_loss : 0.705454\n",
      "batch 13 d_loss : 0.430059\n",
      "batch 13 g_loss : 0.706780\n",
      "batch 14 d_loss : 0.414130\n",
      "batch 14 g_loss : 0.722976\n",
      "batch 15 d_loss : 0.406213\n",
      "batch 15 g_loss : 0.745810\n",
      "batch 16 d_loss : 0.408876\n",
      "batch 16 g_loss : 0.749358\n",
      "batch 17 d_loss : 0.403421\n",
      "batch 17 g_loss : 0.776455\n",
      "batch 18 d_loss : 0.391847\n",
      "batch 18 g_loss : 0.779481\n",
      "batch 19 d_loss : 0.395296\n",
      "batch 19 g_loss : 0.800840\n",
      "batch 20 d_loss : 0.382202\n",
      "batch 20 g_loss : 0.820475\n",
      "batch 21 d_loss : 0.399731\n",
      "batch 21 g_loss : 0.847266\n",
      "batch 22 d_loss : 0.389692\n",
      "batch 22 g_loss : 0.846131\n",
      "batch 23 d_loss : 0.380647\n",
      "batch 23 g_loss : 0.874420\n",
      "batch 24 d_loss : 0.365685\n",
      "batch 24 g_loss : 0.883493\n",
      "batch 25 d_loss : 0.374523\n",
      "batch 25 g_loss : 0.913072\n",
      "batch 26 d_loss : 0.379005\n",
      "batch 26 g_loss : 0.896992\n",
      "batch 27 d_loss : 0.425074\n",
      "batch 27 g_loss : 0.921706\n",
      "batch 28 d_loss : 0.407348\n",
      "batch 28 g_loss : 0.919097\n",
      "batch 29 d_loss : 0.413138\n",
      "batch 29 g_loss : 0.934990\n",
      "batch 30 d_loss : 0.423115\n",
      "batch 30 g_loss : 0.929588\n",
      "batch 31 d_loss : 0.429823\n",
      "batch 31 g_loss : 0.942788\n",
      "batch 32 d_loss : 0.460039\n",
      "batch 32 g_loss : 0.939724\n",
      "batch 33 d_loss : 0.459273\n",
      "batch 33 g_loss : 0.935106\n",
      "batch 34 d_loss : 0.512458\n",
      "batch 34 g_loss : 0.930575\n",
      "batch 35 d_loss : 0.539907\n",
      "batch 35 g_loss : 0.923357\n",
      "batch 36 d_loss : 0.587025\n",
      "batch 36 g_loss : 0.906136\n",
      "batch 37 d_loss : 0.546287\n",
      "batch 37 g_loss : 0.888738\n",
      "batch 38 d_loss : 0.562986\n",
      "batch 38 g_loss : 0.899317\n",
      "batch 39 d_loss : 0.559695\n",
      "batch 39 g_loss : 0.903616\n",
      "batch 40 d_loss : 0.572254\n",
      "batch 40 g_loss : 0.926369\n",
      "batch 41 d_loss : 0.548633\n",
      "batch 41 g_loss : 0.918745\n",
      "batch 42 d_loss : 0.560295\n",
      "batch 42 g_loss : 0.958210\n",
      "batch 43 d_loss : 0.536229\n",
      "batch 43 g_loss : 1.000811\n",
      "batch 44 d_loss : 0.512374\n",
      "batch 44 g_loss : 1.037683\n",
      "batch 45 d_loss : 0.491015\n",
      "batch 45 g_loss : 1.105033\n",
      "batch 46 d_loss : 0.480629\n",
      "batch 46 g_loss : 1.151696\n",
      "batch 47 d_loss : 0.437606\n",
      "batch 47 g_loss : 1.232504\n",
      "batch 48 d_loss : 0.443317\n",
      "batch 48 g_loss : 1.274048\n",
      "batch 49 d_loss : 0.446633\n",
      "batch 49 g_loss : 1.297219\n",
      "batch 50 d_loss : 0.454026\n",
      "batch 50 g_loss : 1.332218\n",
      "batch 51 d_loss : 0.433229\n",
      "batch 51 g_loss : 1.315341\n",
      "batch 52 d_loss : 0.355580\n",
      "batch 52 g_loss : 1.348532\n",
      "batch 53 d_loss : 0.373341\n",
      "batch 53 g_loss : 1.366397\n",
      "batch 54 d_loss : 0.358266\n",
      "batch 54 g_loss : 1.379503\n",
      "batch 55 d_loss : 0.381494\n",
      "batch 55 g_loss : 1.392225\n",
      "batch 56 d_loss : 0.322227\n",
      "batch 56 g_loss : 1.421905\n",
      "batch 57 d_loss : 0.302702\n",
      "batch 57 g_loss : 1.490118\n",
      "batch 58 d_loss : 0.285168\n",
      "batch 58 g_loss : 1.556892\n",
      "batch 59 d_loss : 0.237673\n",
      "batch 59 g_loss : 1.650772\n",
      "batch 60 d_loss : 0.227083\n",
      "batch 60 g_loss : 1.754804\n",
      "batch 61 d_loss : 0.219836\n",
      "batch 61 g_loss : 1.882986\n",
      "batch 62 d_loss : 0.243150\n",
      "batch 62 g_loss : 1.975245\n",
      "batch 63 d_loss : 0.206706\n",
      "batch 63 g_loss : 2.018843\n",
      "batch 64 d_loss : 0.173334\n",
      "batch 64 g_loss : 2.194630\n",
      "batch 65 d_loss : 0.215921\n",
      "batch 65 g_loss : 2.243471\n",
      "batch 66 d_loss : 0.187384\n",
      "batch 66 g_loss : 2.337497\n",
      "batch 67 d_loss : 0.198967\n",
      "batch 67 g_loss : 2.432433\n",
      "batch 68 d_loss : 0.274349\n",
      "batch 68 g_loss : 2.296863\n",
      "batch 69 d_loss : 0.380215\n",
      "batch 69 g_loss : 2.333398\n",
      "batch 70 d_loss : 0.318992\n",
      "batch 70 g_loss : 2.395750\n",
      "batch 71 d_loss : 0.390834\n",
      "batch 71 g_loss : 2.230441\n",
      "batch 72 d_loss : 0.382264\n",
      "batch 72 g_loss : 2.248034\n",
      "batch 73 d_loss : 0.468901\n",
      "batch 73 g_loss : 2.318702\n",
      "batch 74 d_loss : 0.405877\n",
      "batch 74 g_loss : 2.317571\n",
      "batch 75 d_loss : 0.498655\n",
      "batch 75 g_loss : 2.264471\n",
      "batch 76 d_loss : 0.514154\n",
      "batch 76 g_loss : 2.238345\n",
      "batch 77 d_loss : 0.577820\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-f2826d76ddbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m132\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m132\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-f27f450ddccd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(BATCH_SIZE)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_on_g\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(BATCH_SIZE=132)\n",
    "generate(BATCH_SIZE=132)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
