{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [TF-Slim](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim) library provides common abstractions which enable us to define models quickly and concisely, while keeping the model architecture transparent and its hyperparameters explicit. Let's construct models by TF-Slim and see how concisely and beautiful they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/quansun/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np\n",
    "import gzip\n",
    "import os\n",
    "from scipy import ndimage\n",
    "from six.moves import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 28\n",
    "num_channels = 1\n",
    "pixel_depth = 255\n",
    "num_labels = 10\n",
    "validation_size = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_url = 'http://yann.lecun.com/exdb/mnist/'\n",
    "data_dir = './mnist'\n",
    "\n",
    "def maybe_download(filename):\n",
    "    if not tf.gfile.Exists(data_dir):\n",
    "        tf.gfile.MakeDirs(data_dir)\n",
    "    file_path = os.path.join(data_dir, filename)\n",
    "    \n",
    "    if not tf.gfile.Exists(file_path):\n",
    "        file_path, _ = urllib.request.urlretrieve(source_url + filename, file_path)\n",
    "        \n",
    "        with tf.gfile.Gfile(file_path) as f:\n",
    "            size = f.size()\n",
    "        print('Successfully download', filename, size, 'bytes.')\n",
    "    return file_path\n",
    "\n",
    "def extract_data(filename, num_images):\n",
    "    \"\"\"\"Extracting images into a 4D tensor - [image_index,y,x,channels]\"\"\"\n",
    "    print('Extracting', filename)\n",
    "    with gzip.open(filename) as f:\n",
    "        f.read(16)\n",
    "        buf = f.read(image_size * image_size * num_images * num_channels)\n",
    "        data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "        data = (data-(pixel_depth/2.0))/pixel_depth\n",
    "        data = data.reshape(num_images,image_size,image_size,num_channels)\n",
    "        data = np.reshape(data,[num_images,-1])\n",
    "        \n",
    "    return data\n",
    "\n",
    "def extract_labels(filename,num_images):\n",
    "    print('Ectracting', filename)\n",
    "    with gzip.open(filename) as f:\n",
    "        f.read(8)\n",
    "        buf = f.read(1* num_images)\n",
    "        labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n",
    "        num_labels_data = len(labels)\n",
    "        one_hot_encoding = np.zeros((num_labels_data, num_labels))\n",
    "        one_hot_encoding[np.arange(num_labels_data),labels] = 1\n",
    "        one_hot_encoding = np.reshape(one_hot_encoding, [-1,num_labels])\n",
    "        \n",
    "    return one_hot_encoding\n",
    "\n",
    "def expand_training_data(images,labels):\n",
    "    expanded_images = []\n",
    "    expanded_labels = []\n",
    "    \n",
    "    j = 0\n",
    "    for x,y in zip(images,labels):\n",
    "        j += 1\n",
    "        if j%100 == 0:\n",
    "            print('Expanding data: %03d / %03d' % (j, np.size(images,0)))\n",
    "            \n",
    "        expanded_images.append(x)\n",
    "        expanded_labels.append(y)\n",
    "        \n",
    "        background_value = np.median(x)\n",
    "        image = np.reshape(x,(-1,28))\n",
    "        \n",
    "        for i in range(4):\n",
    "            angle = np.random.randint(-15,15,1)\n",
    "            new_image = ndimage.rotate(image,angle,reshape=False,cval=background_value)\n",
    "            \n",
    "            shift = np.random.randint(-2,2,2)\n",
    "            new_image2 = ndimage.shift(new_image, shift, cval=background_value)\n",
    "            \n",
    "            expanded_images.append(np.reshape(new_image2,784))\n",
    "            expanded_labels.append(y)\n",
    "        \n",
    "        expanded_train_total_data = np.concatenate((expanded_images,expanded_labels),axis=1)\n",
    "        np.random.shuffle(expanded_train_total_data)\n",
    "        \n",
    "        return expanded_train_total_data\n",
    "    \n",
    "def prepare_MNIST_data(use_data_augmentation=True):\n",
    "    train_data_filename = maybe_download('train-images-idx3-ubyte.gz')\n",
    "    train_labels_filename = maybe_download('train-labels-idx1-ubyte.gz')\n",
    "    test_data_filename = maybe_download('t10k-images-idx3-ubyte.gz')\n",
    "    test_labels_filename = maybe_download('t10k-labels-idx1-ubyte.gz')\n",
    "    \n",
    "    train_data = extract_data(train_data_filename, 60000)\n",
    "    train_labels = extract_labels(train_labels_filename, 60000)\n",
    "    test_data = extract_data(test_data_filename, 10000)\n",
    "    test_labels = extract_labels(test_labels_filename, 10000)\n",
    "\n",
    "    validation_data = train_data[:validation_size,:]\n",
    "    validation_labels = train_labels[:validation_size,:]\n",
    "    train_data = train_data[validation_size:,:]\n",
    "    train_labels = train_labels[validation_size:,:]\n",
    "    \n",
    "    if use_data_augmentation:\n",
    "        train_total_data = expand_training_data(train_data, train_labels)\n",
    "    else:\n",
    "        train_total_data = np.concatenate((train_data,train_labels),axis=1)\n",
    "        \n",
    "    train_size = train_total_data.shape[0]\n",
    "    return train_total_data,train_size, validation_data, validation_labels, test_data, test_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN(inputs,is_training=True):\n",
    "    batch_norm_params = {'is_training':is_training,'decay':0.9,'updates_collections':None}\n",
    "    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n",
    "                        normalizer_fn=slim.batch_norm,\n",
    "                       normalizer_params=batch_norm_params):\n",
    "        x = tf.reshape(inputs,[-1,28,28,1])\n",
    "        net = slim.conv2d(x,32,[5,5],scope='conv1')\n",
    "        net = slim.max_pool2d(net,[2,2],scope='pool1')\n",
    "        net = slim.conv2d(net,64,[5,5],scope='conv2')\n",
    "        net = slim.max_pool2d(net,[2,2],scope='pool2')\n",
    "        net = slim.flatten(net,scope='flatten3')\n",
    "        \n",
    "        net = slim.fully_connected(net,1024,scope='fc3')\n",
    "        net = slim.dropout(net, is_training=is_training,scope='dropout3')\n",
    "        outputs = slim.fully_connected(net,10,activation_fn=None,normalizer_fn=None,scope='fco')\n",
    "        \n",
    "    return outputs\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/train-images-idx3-ubyte.gz\n",
      "Ectracting ./mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/t10k-images-idx3-ubyte.gz\n",
      "Ectracting ./mnist/t10k-labels-idx1-ubyte.gz\n",
      "Optimizationn finished!\n"
     ]
    }
   ],
   "source": [
    "model_dir = 'model/mnist_model.ckpt'\n",
    "logs_dir = 'logs/train'\n",
    "\n",
    "display_step = 100\n",
    "validation_step = 500\n",
    "\n",
    "def train():\n",
    "    epochs = 1000\n",
    "    batch_size = 32\n",
    "    num_labels = 10\n",
    "    \n",
    "    train_total_data, train_size, validation_data,validation_labels,test_data, test_labels = prepare_MNIST_data(True)\n",
    "    \n",
    "    is_training = tf.placeholder(tf.bool, name='MDOE')\n",
    "    \n",
    "    x = tf.placeholder(tf.float32,[None,784])\n",
    "    y_ = tf.placeholder(tf.float32,[None,10])\n",
    "    \n",
    "    y = CNN(x)\n",
    "    \n",
    "    with tf.name_scope('loss'):\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=y,labels=y_)\n",
    "        \n",
    "    tf.summary.scalar('loss',loss)\n",
    "    \n",
    "    with tf.name_scope('optimizer'):\n",
    "        batch = tf.Variable(0)\n",
    "        learning_rate = tf.train.exponential_decay(1e-4,\n",
    "                                                   batch*batch_size, # current index into the dataset\n",
    "                                                  train_size, # decay step\n",
    "                                                  0.95, # decay rate\n",
    "                                                  staircase=True)\n",
    "        train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss,global_step=batch)\n",
    "        \n",
    "    tf.summary.scalar('learning_rate', learning_rate)\n",
    "    \n",
    "    with tf.name_scope('accuracy'):\n",
    "        correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(y_,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "    tf.summary.scalar('accuracy',accuracy)\n",
    "    \n",
    "    merged_summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer(), feed_dict={is_training:True})\n",
    "    \n",
    "    total_batch = int(train_size/batch_size)\n",
    "    \n",
    "    if not tf.gfile.Exists(logs_dir):\n",
    "        tf.gfile.MakeDirs(logs_dir)\n",
    "        \n",
    "    if not tf.gfile.Exists(model_dir):\n",
    "        tf.gfile.MakeDirs(model_dir)\n",
    "    \n",
    "    summary_writer = tf.summary.FileWriter(logs_dir, graph=tf.get_default_graph())\n",
    "    \n",
    "    max_acc = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        np.random.shuffle(train_total_data)\n",
    "        train_data_ = train_total_data[:, :-num_labels]\n",
    "        train_labels_ = train_total_data[:, -num_labels:]\n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            offset = (i*batch_size)%(train_size)\n",
    "            batch_x = train_data_[offset:(offset+batch_size), :]\n",
    "            batch_y = train_labels_[offset:(offset+batch_size), :]\n",
    "            \n",
    "            _, train_accuracy, summary = sess.run([train_step,accuracy, merged_summary_op],feed_dict={x:batch_x,y_:batch_y,is_training:True})\n",
    "            \n",
    "            summary_writer.add_summary(summary, epoch*total_batch+1)\n",
    "            \n",
    "            if i%display_step == 0:\n",
    "                print('Epoch: %04d, batch_index: %4d/%4d, training_accuracy: %.5f'%(epoch+1, i, total_batch, train_accuracy))\n",
    "                \n",
    "            if i%validation_step == 0:\n",
    "                validation_accuracy = sess.run(accuracy,feed_dict={x:validation_data,y_:validation_labels,is_training:False})\n",
    "                print('Epoch: %04d, batch_index: %4d/%4d, validation_accuracy: %.5f'%(epoch+1, i, total_batch, validation_accuracy))\n",
    "                \n",
    "            if validation_accuracy > max_acc:\n",
    "                max_acc = validation_accuracy\n",
    "                save_path = saver.save(sess, model_dir)\n",
    "    print('Optimizationn finished!')\n",
    "    \n",
    "#     saver.restore(sess, model_dir)\n",
    "    \n",
    "    test_size = test_labels.shape[0]\n",
    "    acc_buffer= []\n",
    "    \n",
    "    y_final = sess.run(y, feed_dict={x:test_data,y_:test_labels, is_training:False})\n",
    "    correct_prediction = tf.equal(tf.argmax(y_final,1),tf.argmax(test_labels,1))\n",
    "    acc_buffer.append(tf.reduce_mean(tf.cast(correct_prediction,tf.float32)))\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
