{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation is based on [tdeboissiere](https://github.com/tdeboissiere/DeepLearningImplementations/blob/master/DenseNet/densenet.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/quansun/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers.pooling import AveragePooling2D\n",
    "from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from keras.layers import Input, Concatenate\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(x, concat_axis, nb_filter,\n",
    "                 dropout_rate=None, weight_decay=1e-4):\n",
    "\n",
    "    x = BatchNormalization(axis=concat_axis,\n",
    "                           gamma_regularizer=l2(weight_decay),\n",
    "                           beta_regularizer=l2(weight_decay))(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(nb_filter, (3, 3),\n",
    "               kernel_initializer=\"he_uniform\",\n",
    "               padding=\"same\",\n",
    "               use_bias=False,\n",
    "               kernel_regularizer=l2(weight_decay))(x)\n",
    "    if dropout_rate:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def transition_block(x, concat_axis, nb_filter,\n",
    "               dropout_rate=None, weight_decay=1E-4):\n",
    "\n",
    "\n",
    "    x = BatchNormalization(axis=concat_axis,\n",
    "                           gamma_regularizer=l2(weight_decay),\n",
    "                           beta_regularizer=l2(weight_decay))(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(nb_filter, (1, 1),\n",
    "               kernel_initializer=\"he_uniform\",\n",
    "               padding=\"same\",\n",
    "               use_bias=False,\n",
    "               kernel_regularizer=l2(weight_decay))(x)\n",
    "    if dropout_rate:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    x = AveragePooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def denseblock(x, concat_axis, nb_layers, nb_filter, growth_rate,\n",
    "               dropout_rate=None, weight_decay=1e-4):\n",
    "\n",
    "    list_feat = [x]\n",
    "\n",
    "    for i in range(nb_layers):\n",
    "        x = conv_block(x, concat_axis, growth_rate,\n",
    "                         dropout_rate, weight_decay)\n",
    "        list_feat.append(x)\n",
    "        x = Concatenate(axis=concat_axis)(list_feat)\n",
    "        nb_filter += growth_rate\n",
    "\n",
    "    return x, nb_filter\n",
    "\n",
    "\n",
    "# def denseblock(x, concat_axis, nb_layers, nb_filter, growth_rate,\n",
    "#                       dropout_rate=None, weight_decay=1e-4):\n",
    "\n",
    "#     for i in range(nb_layers):\n",
    "#         merge_tensor = conv_block(x, concat_axis, growth_rate,\n",
    "#                                     dropout_rate, weight_decay)\n",
    "#         x = Concatenate(axis=concat_axis)([merge_tensor, x])\n",
    "#         nb_filter += growth_rate\n",
    "\n",
    "#     return x, nb_filter\n",
    "\n",
    "\n",
    "def DenseNet(nb_classes, img_dim, depth, nb_dense_block, growth_rate,\n",
    "             nb_filter, dropout_rate=None, weight_decay=1e-4):\n",
    "\n",
    "    if K.image_dim_ordering() == \"tf\":\n",
    "        concat_axis = -1\n",
    "    elif K.image_dim_ordering() == \"th\":\n",
    "        concat_axis = 1\n",
    "\n",
    "    model_input = Input(shape=img_dim)\n",
    "\n",
    "    assert (depth - 4) % 3 == 0, \"Depth must be 3 N + 4\"\n",
    "\n",
    "    # layers in each dense block\n",
    "    nb_layers = int((depth - 4) / 3)\n",
    "\n",
    "    # Initial convolution\n",
    "    x = Conv2D(nb_filter, (3, 3),\n",
    "               kernel_initializer=\"he_uniform\",\n",
    "               padding=\"same\",\n",
    "               name=\"initial_conv2D\",\n",
    "               use_bias=False,\n",
    "               kernel_regularizer=l2(weight_decay))(model_input)\n",
    "\n",
    "    # Add dense blocks\n",
    "    for block_idx in range(nb_dense_block - 1):\n",
    "        x, nb_filter = denseblock(x, concat_axis, nb_layers,\n",
    "                                  nb_filter, growth_rate, \n",
    "                                  dropout_rate=dropout_rate,\n",
    "                                  weight_decay=weight_decay)\n",
    "        # add transition\n",
    "        x = transition_block(x, nb_filter, dropout_rate=dropout_rate,\n",
    "                       weight_decay=weight_decay)\n",
    "\n",
    "    # The last denseblock does not have a transition\n",
    "    x, nb_filter = denseblock(x, concat_axis, nb_layers,\n",
    "                              nb_filter, growth_rate, \n",
    "                              dropout_rate=dropout_rate,\n",
    "                              weight_decay=weight_decay)\n",
    "\n",
    "    x = BatchNormalization(axis=concat_axis,\n",
    "                           gamma_regularizer=l2(weight_decay),\n",
    "                           beta_regularizer=l2(weight_decay))(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = GlobalAveragePooling2D(data_format=K.image_data_format())(x)\n",
    "    x = Dense(nb_classes,\n",
    "              activation='softmax',\n",
    "              kernel_regularizer=l2(weight_decay),\n",
    "              bias_regularizer=l2(weight_decay))(x)\n",
    "\n",
    "    densenet = Model(inputs=[model_input], outputs=[x], name=\"DenseNet\")\n",
    "\n",
    "    return densenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DenseNet implemented in Keras\n",
    "\n",
    "This implementation is based on the original paper of Gao Huang, Zhuang Liu, Kilian Q. Weinberger and Laurens van der Maaten.\n",
    "Besides I took some influences by random implementations, especially of Zhuang Liu's Lua implementation.\n",
    "\n",
    "**  References**\n",
    "- [Densely Connected Convolutional Networks](https://arxiv.org/abs/1608.06993)\n",
    "- [DenseNet - Lua implementation](https://github.com/liuzhuang13/DenseNet)\n",
    "\n",
    "\n",
    "@author: Christopher Masch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Activation, Convolution2D, Dropout, GlobalAveragePooling2D, Concatenate, Dense, Input, AveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "\n",
    "def DenseNet(\n",
    "    input_shape=None,\n",
    "    dense_blocks=3,\n",
    "    dense_layers=-1,\n",
    "    growth_rate=12,\n",
    "    nb_classes=None,\n",
    "    dropout_rate=None,\n",
    "    bottleneck=False,\n",
    "    compression=1.0,\n",
    "    weight_decay=1e-4,\n",
    "    depth=40):\n",
    "   \n",
    "    if nb_classes==None:\n",
    "        raise Exception('Please define number of classes (e.g. num_classes=10). This is required for final softmax.')\n",
    "    \n",
    "    if compression <=0.0 or compression > 1.0:\n",
    "        raise Exception('Compression have to be a value between 0.0 and 1.0.')\n",
    "    \n",
    "    if type(dense_layers) is list:\n",
    "        if len(dense_layers) != dense_blocks:\n",
    "            raise AssertionError('Number of dense blocks have to be same length to specified layers')\n",
    "    elif dense_layers == -1:\n",
    "        dense_layers = int((depth - 4)/3)\n",
    "        if bottleneck:\n",
    "            dense_layers = int(dense_layers / 2)\n",
    "        dense_layers = [dense_layers for _ in range(dense_blocks)]\n",
    "    else:\n",
    "        dense_layers = [dense_layers for _ in range(dense_blocks)]\n",
    "        \n",
    "    img_input = Input(shape=input_shape)\n",
    "    nb_channels = growth_rate\n",
    "    \n",
    "    print('Creating DenseNet %s' % __version__)\n",
    "    print('#############################################')\n",
    "    print('Dense blocks: %s' % dense_blocks)\n",
    "    print('Layers per dense block: %s' % dense_layers)\n",
    "    print('#############################################')\n",
    "    \n",
    "    # Initial convolution layer\n",
    "    x = Convolution2D(2 * growth_rate, (3,3), padding='same',strides=(1,1),\n",
    "                      use_bias=False, kernel_regularizer=l2(weight_decay))(img_input)\n",
    "    \n",
    "    # Building dense blocks\n",
    "    for block in range(dense_blocks - 1):\n",
    "        \n",
    "        # Add dense block\n",
    "        x, nb_channels = dense_block(x, dense_layers[block], nb_channels, growth_rate, dropout_rate, bottleneck, weight_decay)\n",
    "        \n",
    "        # Add transition_block\n",
    "        x = transition_layer(x, nb_channels, dropout_rate, compression, weight_decay)\n",
    "        nb_channels = int(nb_channels * compression)\n",
    "    \n",
    "    # Add last dense block without transition but for that with global average pooling\n",
    "    x, nb_channels = dense_block(x, dense_layers[-1], nb_channels, growth_rate, dropout_rate, weight_decay)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    x = Dense(nb_classes, activation='softmax')(x)\n",
    "    \n",
    "    return Model(img_input, x, name='densenet')\n",
    "\n",
    "\n",
    "def dense_block(x, nb_layers, nb_channels, growth_rate, dropout_rate=None, bottleneck=False, weight_decay=1e-4):\n",
    "    \"\"\"\n",
    "    Creates a dense block and concatenates inputs\n",
    "    \"\"\"\n",
    "    \n",
    "    x_list = [x]\n",
    "    for i in range(nb_layers):\n",
    "        cb = convolution_block(x, growth_rate, dropout_rate, bottleneck)\n",
    "        x_list.append(cb)\n",
    "        x = Concatenate(axis=-1)(x_list)\n",
    "        nb_channels += growth_rate\n",
    "    return x, nb_channels\n",
    "\n",
    "\n",
    "def convolution_block(x, nb_channels, dropout_rate=None, bottleneck=False, weight_decay=1e-4):\n",
    "    \n",
    "    # Bottleneck\n",
    "    if bottleneck:\n",
    "        bottleneckWidth = 4\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Convolution2D(nb_channels * bottleneckWidth, (1, 1), use_bias=False, kernel_regularizer=l2(weight_decay))(x)\n",
    "        # Dropout\n",
    "        if dropout_rate:\n",
    "            x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # Standard (BN-ReLU-Conv)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Convolution2D(nb_channels, (3, 3), padding='same', use_bias=False)(x)\n",
    "    \n",
    "    # Dropout\n",
    "    if dropout_rate:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def transition_layer(x, nb_channels, dropout_rate=None, compression=1.0, weight_decay=1e-4):\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Convolution2D(int(nb_channels*compression), (1, 1), padding='same',\n",
    "                      use_bias=False, kernel_regularizer=l2(weight_decay))(x)\n",
    "    \n",
    "    # Adding dropout\n",
    "    if dropout_rate:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    x = AveragePooling2D((2, 2), strides=(2, 2))(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
