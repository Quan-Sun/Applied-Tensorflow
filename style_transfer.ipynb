{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "style_transfer.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Quan-Sun/Learning_Tensorflow/blob/master/style_transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "KmzRaBp3f7XE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A tensorflow implementation of style transfer described in the papers **[Image Style Transfer Using Convolutional Neural Networks](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf)**\n",
        "\n",
        "Most code in this file was borrowed from https://github.com/hwalsuklee/tensorflow-style-transfer"
      ]
    },
    {
      "metadata": {
        "id": "XaYMZn8ZqMQM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "args_content = 'picture.jpeg'  # choose the image that you want transfer\n",
        "args_style = 'portrait.jpg' # choose the image that you love its style\n",
        "\n",
        "from google.colab import files\n",
        "files.upload()     # choose the file- args_content and args_style"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DvguSTsWf7XF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ca21bba4-24f7-47d8-9a50-806c35efa798"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf \n",
        "import numpy as np \n",
        "import scipy.io \n",
        "from six.moves import urllib\n",
        "import os\n",
        "\n",
        "source_url = 'http://www.vlfeat.org/matconvnet/models/imagenet-vgg-verydeep-19.mat'\n",
        "data_dir = './pre_trained_model'\n",
        "filename = 'imagenet-vgg-verydeep-19.mat'\n",
        "def maybe_download(filename):\n",
        "    if not tf.gfile.Exists(data_dir):\n",
        "        tf.gfile.MakeDirs(data_dir)\n",
        "    file_path = os.path.join(data_dir, filename)\n",
        "    \n",
        "    if not tf.gfile.Exists(file_path):\n",
        "        file_path, _ = urllib.request.urlretrieve(source_url, file_path)\n",
        "        \n",
        "        with tf.gfile.GFile(file_path) as f:\n",
        "            size = f.size()\n",
        "        print('Successfully download', filename, size, 'bytes.')\n",
        "    return file_path\n",
        "\n",
        "model_filename = maybe_download(filename)\n",
        "\n",
        "def _conv_layer(input, weights, bias,padding='SAME'):\n",
        "    conv = tf.nn.conv2d(input,tf.constant(weights),strides=[1,1,1,1],padding= padding)\n",
        "    h_conv = conv + bias\n",
        "\n",
        "    return h_conv\n",
        "\n",
        "def _pool_layer(input, padding='SAME'):\n",
        "    h_pool = tf.nn.max_pool(input, ksize=[1,2,2,1], strides=[1,2,2,1],padding= padding)\n",
        "\n",
        "    return h_pool\n",
        "\n",
        "def preprocess(image, mean_pixel):\n",
        "    return image - mean_pixel\n",
        "\n",
        "def unpreprocess(image, mean_pixel):\n",
        "    return image + mean_pixel\n",
        "\n",
        "class VGG19:\n",
        "    layers = (\n",
        "        'conv1_1', 'relu1_1', 'conv1_2', 'relu1_2', 'pool1',\n",
        "        'conv2_1', 'relu2_1', 'conv2_2', 'relu2_2', 'pool2',\n",
        "        'conv3_1', 'relu3_1', 'conv3_2', 'relu3_2', 'conv3_3', 'relu3_3', 'conv3_4', 'relu3_4', 'pool3',\n",
        "        'conv4_1', 'relu4_1', 'conv4_2', 'relu4_2', 'conv4_3', 'relu4_3', 'conv4_4', 'relu4_4', 'pool4',\n",
        "        'conv5_1', 'relu5_1', 'conv5_2', 'relu5_2', 'conv5_3', 'relu5_3', 'conv5_4', 'relu5_4'\n",
        "        )\n",
        "\n",
        "    def __init__(self, model_filename):\n",
        "        model = scipy.io.loadmat(model_filename)\n",
        "\n",
        "        self.mean_pixel = np.array([123.68, 116.779, 103.939]) #np.mean(model['normalization'][0][0][0], axis=(0,1)) \n",
        "\n",
        "        self.weights = model['layers'][0]\n",
        "\n",
        "    def preprocess(self, image):\n",
        "        return np.float32(image - self.mean_pixel)\n",
        "\n",
        "    def unpreprocess(self, image):\n",
        "        return np.float32(image + self.mean_pixel)\n",
        "\n",
        "    def feed_forward(self, input_image, scope=None):\n",
        "        current_network = {}\n",
        "        current_layer = input_image\n",
        "\n",
        "        with tf.variable_scope(scope):\n",
        "            for num, name in enumerate(self.layers):\n",
        "                type_layer = name[:4]\n",
        "                if type_layer == 'conv':\n",
        "                    kernels = self.weights[num][0][0][2][0][0]\n",
        "                    bias = self.weights[num][0][0][2][0][1]\n",
        "\n",
        "                    # vgg19: shape of weights is [width, height, in_channels, out_channels]\n",
        "                    # tensorflow: shape of weights is [height, width, in_channels, out_channels]\n",
        "\n",
        "                    kernels = np.transpose(kernels, [1,0,2,3])\n",
        "                    bias = bias.reshape(-1)\n",
        "                    current_layer = _conv_layer(current_layer, kernels, bias)\n",
        "\n",
        "                elif type_layer == 'relu':\n",
        "                    current_layer = tf.nn.relu(current_layer)\n",
        "\n",
        "                elif type_layer == 'pool':\n",
        "                    current_layer = _pool_layer(current_layer)\n",
        "\n",
        "                current_network[name] = current_layer\n",
        "        assert len(current_network) == len(self.layers)\n",
        "        return current_network"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully download imagenet-vgg-verydeep-19.mat 534904783 bytes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "P6CcprPjf7XM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt \n",
        "import PIL\n",
        "\n",
        "def load_image(filename, assign_shape=None, max_size=None):\n",
        "    image = PIL.Image.open(filename)\n",
        "\n",
        "    if max_size:\n",
        "        proportion = float(max_size)/np.max(image.size)\n",
        "        size = np.array(image.size) * proportion\n",
        "\n",
        "        # PIL manipulation needs the size to be integers\n",
        "        size = size.astype(int)\n",
        "\n",
        "        # Resize the image\n",
        "        image = image.resize(size, PIL.Image.LANCZOS) # PIL.Image.LANCZOS is a resampling filter\n",
        "\n",
        "    if assign_shape:\n",
        "        image = image.resize(assign_shape, PIL.Image.LANCZOS)\n",
        "\n",
        "    image = np.float32(image)\n",
        "    return image\n",
        "\n",
        "# Save images as files of *.jpeg\n",
        "def save_image(image,filename):\n",
        "    image = np.clip(image, 0.0, 255.0)\n",
        "\n",
        "    image = image.astype(np.uint8) # convert float to bytes\n",
        "\n",
        "    with open(filename, 'wb') as f:\n",
        "        PIL.Image.fromarray(image).save(f, 'jpeg')\n",
        "\n",
        "\n",
        "# DRAW the content-, mixed-, style-images\n",
        "def draw_images(content_image, style_image, mixed_image):\n",
        "    fig,axes = plt.subplots(1,3,figsize=(10,10))\n",
        "\n",
        "    fig.subplots_adjust(hspace=0.1,wspace=0.1)\n",
        "\n",
        "    ax = axes.flat[0]\n",
        "    ax.imshow(content_image/255.0, interpolation='sinc')\n",
        "    ax.set_xlabel('Content')\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "\n",
        "    ax = axes.flat[1]\n",
        "    ax.imshow(mixed_image/255.0, interpolation='sinc')\n",
        "    ax.set_xlabel('Output')\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "\n",
        "    ax = axes.flat[2]\n",
        "    ax.imshow(style_image/255.0, interpolation='sinc')\n",
        "    ax.set_xlabel('Style')\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JBOJ5_tjf7XO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "class StyleTransfer:\n",
        "\n",
        "    def __init__(self, content_layer, style_layer, init_image, content_image, style_image,\n",
        "                session, model_selection, num_iter, loss_ratio, content_loss_norm_type):\n",
        "\n",
        "        self.model_selection = model_selection\n",
        "        self.sess = session\n",
        "\n",
        "        self.CONTENT_LAYERS = collections.OrderedDict(sorted(content_layer.items()))\n",
        "        self.STYLE_LAYERS = collections.OrderedDict(sorted(style_layer.items()))\n",
        "\n",
        "        # Preprocess\n",
        "        self.content_image_preprocess = self.model_selection.preprocess(content_image)\n",
        "        self.style_image_preprocess = self.model_selection.preprocess(style_image)\n",
        "        self.init_image_preprocess = self.model_selection.preprocess(init_image)\n",
        "\n",
        "        # Parameters for optimization\n",
        "        self.content_loss_norm_type = content_loss_norm_type\n",
        "        self.num_iter = num_iter\n",
        "        self.loss_ratio = loss_ratio\n",
        "        self._build_graph()\n",
        "\n",
        "\n",
        "    def _gram_matrix(self, tensor):\n",
        "        shape = tensor.get_shape()\n",
        "        num_channels = int(shape[3])\n",
        "        matrix = tf.reshape(tensor, shape=[-1,num_channels])\n",
        "        gram = tf.matmul(tf.transpose(matrix), matrix)\n",
        "        return gram\n",
        "\n",
        "\n",
        "    def _build_graph(self):\n",
        "        self.init_image_variable = tf.Variable(self.init_image_preprocess, trainable=True, dtype=tf.float32)\n",
        "\n",
        "        self.input_content_image = tf.placeholder(tf.float32, shape=self.content_image_preprocess.shape, name='content')\n",
        "        self.output_style_image = tf.placeholder(tf.float32, shape=self.style_image_preprocess.shape, name='style')\n",
        "\n",
        "        content_layers = self.model_selection.feed_forward(self.input_content_image, scope='content')\n",
        "        self.content_features = {}\n",
        "        for layer in self.CONTENT_LAYERS:\n",
        "            self.content_features[layer] = content_layers[layer]\n",
        "\n",
        "        style_layers = self.model_selection.feed_forward(self.output_style_image, scope='style')\n",
        "        self.style_features = {}\n",
        "        for layer in self.STYLE_LAYERS:\n",
        "            self.style_features[layer] = self._gram_matrix(style_layers[layer])\n",
        "\n",
        "        self.init_featues = self.model_selection.feed_forward(self.init_image_variable, scope='mixed')\n",
        "\n",
        "        Loss_content = 0\n",
        "        Loss_style = 0\n",
        "        for layer in self.init_featues:\n",
        "            if layer in self.CONTENT_LAYERS:\n",
        "                init_featues_value = self.init_featues[layer] \n",
        "                content_features_value = self.content_features[layer] \n",
        "\n",
        "                _, heighgt, width, num_filters = init_featues_value.get_shape()\n",
        "                N = heighgt.value * width.value\n",
        "                M = num_filters.value # number of filters\n",
        "\n",
        "                W = self.CONTENT_LAYERS[layer]\n",
        "\n",
        "                if self.content_loss_norm_type==1:\n",
        "                    Loss_content += w * tf.reduce_sum(tf.pow((init_featues_value - content_features_value),2))/2\n",
        "                elif self.content_loss_norm_type==2:\n",
        "                    Loss_content += w * tf.reduce_sum(tf.pow((init_featues_value - content_features_value),2))/(N*M)\n",
        "\n",
        "                elif self.content_loss_norm_type==3:\n",
        "                    Loss_content += w * (1. / (2. * np.sqrt(M) * np.sqrt(N))) * tf.reduce_sum(tf.pow((init_featues_value - content_features_value),2))\n",
        "\n",
        "            elif layer in self.STYLE_LAYERS:\n",
        "                init_featues_value = self.init_featues[layer]\n",
        "\n",
        "                _,h,w,d = init_featues_value.get_shape()\n",
        "                N = h.value * w.value\n",
        "                M = d.value\n",
        "\n",
        "                w = self.STYLE_LAYERS[layer]\n",
        "                G = self._gram_matrix(init_featues_value)\n",
        "                A = self.style_features[layer]\n",
        "\n",
        "                Loss_style += w * (1. / (4. * N ** 2 * M ** 2)) * tf.reduce_sum(tf.pow((G-A),2))\n",
        "\n",
        "        alpha = self.loss_ratio\n",
        "        beta = 1\n",
        "\n",
        "        self.Loss_content = Loss_content\n",
        "        self.Loss_style = Loss_style\n",
        "        self.Loss_total = alpha*Loss_content + beta*Loss_style\n",
        "\n",
        "\n",
        "    def optimize(self):\n",
        "        # define optimizer L-BFGS\n",
        "        global iteration\n",
        "        iteration = 0\n",
        "        def callback(total_loss, content_loss, style_loss):\n",
        "            global iteration\n",
        "            print('iteration: %4d, '%iteration, 'Loss_total: %g, Loss_content: %g, Loss_style: %g' % (total_loss, content_loss, style_loss))\n",
        "            iteration += 1\n",
        "\n",
        "        optimizer = tf.contrib.opt.ScipyOptimizerInterface(self.Loss_total, method='L-BFGS-B', options={'maxiter':self.num_iter})\n",
        "\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        optimizer.minimize(self.sess, feed_dict={self.output_style_image:self.style_image_preprocess, self.input_content_image:self.content_image_preprocess},\n",
        "            fetches=[self.Loss_total, self.Loss_content, self.Loss_style], loss_callback=callback)\n",
        "\n",
        "        final_image = self.sess.run(self.init_image_variable)\n",
        "        final_image = np.clip(self.model_selection.unpreprocess(final_image), 0.0, 255.0)\n",
        "\n",
        "        return final_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tDNixojSf7XR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17731
        },
        "outputId": "8ce5a905-81bd-4768-ce02-7c070db97a85"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "args_output = 'mixed_image.jpg'\n",
        "args_loss_ratio = 1e-3\n",
        "args_content_layers = ['conv4_2']\n",
        "args_style_layers = ['relu1_1', 'relu2_1', 'relu3_1', 'relu4_1', 'relu5_1']\n",
        "args_content_layer_weights = [1.0]\n",
        "args_style_layer_weights = [.2,.2,.2,.2,.2]\n",
        "args_initial_type = 'content' # choices=['random','content','style']\n",
        "args_max_size = 1024\n",
        "args_content_loss_norm_type = 3 #choices=[1,2,3]\n",
        "args_num_iter = 1000\n",
        "\n",
        "\n",
        "\n",
        "try:\n",
        "    assert len(args_content_layers) == len(args_content_layer_weights)\n",
        "except:\n",
        "    print('Content layers info and weights info must be matched')\n",
        "\n",
        "try:\n",
        "    assert len(args_style_layers) == len(args_style_layer_weights)\n",
        "except:\n",
        "    print('Style layers info and weight info must be matched')\n",
        "\n",
        "\n",
        "try:\n",
        "    assert args_max_size>100\n",
        "\n",
        "except:\n",
        "    print('Too small size')\n",
        "\n",
        "\n",
        "model_file_path = model_filename\n",
        "\n",
        "try:\n",
        "    assert os.path.exists(model_file_path)\n",
        "except:\n",
        "    print('There is no %s' % model_file_path)\n",
        "\n",
        "\n",
        "try :\n",
        "    size_in_KB = os.path.getsize(model_file_path)\n",
        "    assert abs(size_in_KB - 534904783) < 10\n",
        "except:\n",
        "    print(\"Check file size of 'imagenet-vgg-verydeep-19.mat' \")\n",
        "    print('There are some files with the same name')\n",
        "    print('pre_trained_model used here can be download from below')\n",
        "    print('http://www.vlfeat.org/matconvnet/models/imagenet-vgg-verydeep-19.mat')\n",
        "\n",
        "\n",
        "try:\n",
        "    assert os.path.exists(args_content)\n",
        "except:\n",
        "    print('There is no %s'%args_content)\n",
        "\n",
        "\n",
        "try:\n",
        "    assert os.path.exists(args_style)\n",
        "except:\n",
        "    print('There is no %s'%args_style)\n",
        "\n",
        "\n",
        "# VGG19 requires input dimension to be [batch, height, width, channel]\n",
        "\n",
        "\n",
        "def add_one_dim(image):\n",
        "    shape = (1,) + image.shape\n",
        "    return np.reshape(image,shape)\n",
        "\n",
        "model_file_path = model_filename\n",
        "vgg_net = VGG19(model_file_path)\n",
        "\n",
        "content_image = load_image(args_content, max_size=args_max_size)\n",
        "style_image = load_image(args_style, assign_shape=[content_image.shape[1],content_image.shape[0]])\n",
        "\n",
        "if args_initial_type == 'content':\n",
        "    initial_image = content_image\n",
        "elif args_initial_type == 'style':\n",
        "    initial_image = style_image\n",
        "elif args_initial_type == 'random':\n",
        "    initial_image = np.ranodm.normal(size=content_image.shape, scale=np.std(content_image))\n",
        "\n",
        "CONTENT_LAYERS = {}\n",
        "for layer, weight in zip(args_content_layers, args_content_layer_weights):\n",
        "    CONTENT_LAYERS[layer] = weight\n",
        "\n",
        "STYLE_LAYERS = {}\n",
        "for layer, weight in zip(args_style_layers, args_style_layer_weights):\n",
        "    STYLE_LAYERS[layer] = weight\n",
        "\n",
        "sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
        "\n",
        "style_teansfer = StyleTransfer(session=sess,\n",
        "                content_layer=CONTENT_LAYERS,\n",
        "                style_layer=STYLE_LAYERS,\n",
        "                init_image = add_one_dim(initial_image),\n",
        "                content_image = add_one_dim(content_image),\n",
        "                style_image = add_one_dim(style_image),\n",
        "                model_selection=vgg_net,\n",
        "                num_iter = args_num_iter,\n",
        "                loss_ratio = args_loss_ratio,\n",
        "                content_loss_norm_type = args_content_loss_norm_type,\n",
        "                )\n",
        "\n",
        "result_image = style_teansfer.optimize()\n",
        "sess.close()\n",
        "\n",
        "shape = result_image.shape\n",
        "result_image = np.reshape(result_image, shape[1:])\n",
        "\n",
        "save_image(result_image, args_output)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration:    0,  Loss_total: 1.05573e+09, Loss_content: 0, Loss_style: 1.05573e+09\n",
            "iteration:    1,  Loss_total: 1.0554e+09, Loss_content: 22.6252, Loss_style: 1.0554e+09\n",
            "iteration:    2,  Loss_total: 1.05406e+09, Loss_content: 567.662, Loss_style: 1.05406e+09\n",
            "iteration:    3,  Loss_total: 1.04874e+09, Loss_content: 10122.1, Loss_style: 1.04874e+09\n",
            "iteration:    4,  Loss_total: 1.02757e+09, Loss_content: 169139, Loss_style: 1.02757e+09\n",
            "iteration:    5,  Loss_total: 9.50359e+08, Loss_content: 2.44104e+06, Loss_style: 9.50357e+08\n",
            "iteration:    6,  Loss_total: 5.76628e+08, Loss_content: 5.4265e+07, Loss_style: 5.76573e+08\n",
            "iteration:    7,  Loss_total: 3.12687e+08, Loss_content: 9.85602e+07, Loss_style: 3.12588e+08\n",
            "iteration:    8,  Loss_total: 1.82608e+08, Loss_content: 1.4817e+08, Loss_style: 1.8246e+08\n",
            "iteration:    9,  Loss_total: 1.3821e+08, Loss_content: 1.40796e+08, Loss_style: 1.38069e+08\n",
            "iteration:   10,  Loss_total: 1.10629e+08, Loss_content: 1.42601e+08, Loss_style: 1.10487e+08\n",
            "iteration:   11,  Loss_total: 8.91232e+07, Loss_content: 1.47911e+08, Loss_style: 8.89753e+07\n",
            "iteration:   12,  Loss_total: 7.01803e+07, Loss_content: 1.52102e+08, Loss_style: 7.00282e+07\n",
            "iteration:   13,  Loss_total: 4.63013e+07, Loss_content: 1.61283e+08, Loss_style: 4.614e+07\n",
            "iteration:   14,  Loss_total: 3.73498e+07, Loss_content: 1.63091e+08, Loss_style: 3.71867e+07\n",
            "iteration:   15,  Loss_total: 3.25738e+07, Loss_content: 1.64243e+08, Loss_style: 3.24095e+07\n",
            "iteration:   16,  Loss_total: 2.57141e+07, Loss_content: 1.68737e+08, Loss_style: 2.55454e+07\n",
            "iteration:   17,  Loss_total: 2.50476e+07, Loss_content: 1.71336e+08, Loss_style: 2.48763e+07\n",
            "iteration:   18,  Loss_total: 2.15034e+07, Loss_content: 1.65245e+08, Loss_style: 2.13382e+07\n",
            "iteration:   19,  Loss_total: 2.03556e+07, Loss_content: 1.67964e+08, Loss_style: 2.01876e+07\n",
            "iteration:   20,  Loss_total: 1.9073e+07, Loss_content: 1.68449e+08, Loss_style: 1.89046e+07\n",
            "iteration:   21,  Loss_total: 1.55188e+07, Loss_content: 1.71105e+08, Loss_style: 1.53477e+07\n",
            "iteration:   22,  Loss_total: 1.6511e+07, Loss_content: 1.71327e+08, Loss_style: 1.63397e+07\n",
            "iteration:   23,  Loss_total: 1.42054e+07, Loss_content: 1.71036e+08, Loss_style: 1.40343e+07\n",
            "iteration:   24,  Loss_total: 1.305e+07, Loss_content: 1.70969e+08, Loss_style: 1.2879e+07\n",
            "iteration:   25,  Loss_total: 1.20377e+07, Loss_content: 1.72632e+08, Loss_style: 1.1865e+07\n",
            "iteration:   26,  Loss_total: 1.13121e+07, Loss_content: 1.71778e+08, Loss_style: 1.11403e+07\n",
            "iteration:   27,  Loss_total: 1.0422e+07, Loss_content: 1.7308e+08, Loss_style: 1.02489e+07\n",
            "iteration:   28,  Loss_total: 9.92836e+06, Loss_content: 1.73224e+08, Loss_style: 9.75513e+06\n",
            "iteration:   29,  Loss_total: 9.03289e+06, Loss_content: 1.75973e+08, Loss_style: 8.85692e+06\n",
            "iteration:   30,  Loss_total: 8.42032e+06, Loss_content: 1.75942e+08, Loss_style: 8.24438e+06\n",
            "iteration:   31,  Loss_total: 8.1243e+06, Loss_content: 1.76746e+08, Loss_style: 7.94755e+06\n",
            "iteration:   32,  Loss_total: 7.79662e+06, Loss_content: 1.77271e+08, Loss_style: 7.61935e+06\n",
            "iteration:   33,  Loss_total: 7.32836e+06, Loss_content: 1.77884e+08, Loss_style: 7.15048e+06\n",
            "iteration:   34,  Loss_total: 6.9121e+06, Loss_content: 1.78979e+08, Loss_style: 6.73312e+06\n",
            "iteration:   35,  Loss_total: 6.53602e+06, Loss_content: 1.79038e+08, Loss_style: 6.35698e+06\n",
            "iteration:   36,  Loss_total: 6.36431e+06, Loss_content: 1.79469e+08, Loss_style: 6.18484e+06\n",
            "iteration:   37,  Loss_total: 6.09231e+06, Loss_content: 1.79779e+08, Loss_style: 5.91253e+06\n",
            "iteration:   38,  Loss_total: 5.91533e+06, Loss_content: 1.81761e+08, Loss_style: 5.73357e+06\n",
            "iteration:   39,  Loss_total: 5.58762e+06, Loss_content: 1.81907e+08, Loss_style: 5.40572e+06\n",
            "iteration:   40,  Loss_total: 5.37632e+06, Loss_content: 1.81982e+08, Loss_style: 5.19433e+06\n",
            "iteration:   41,  Loss_total: 5.19211e+06, Loss_content: 1.83153e+08, Loss_style: 5.00896e+06\n",
            "iteration:   42,  Loss_total: 4.91531e+06, Loss_content: 1.84056e+08, Loss_style: 4.73126e+06\n",
            "iteration:   43,  Loss_total: 4.8709e+06, Loss_content: 1.86136e+08, Loss_style: 4.68476e+06\n",
            "iteration:   44,  Loss_total: 4.6649e+06, Loss_content: 1.84615e+08, Loss_style: 4.48028e+06\n",
            "iteration:   45,  Loss_total: 4.6155e+06, Loss_content: 1.85142e+08, Loss_style: 4.43036e+06\n",
            "iteration:   46,  Loss_total: 4.52242e+06, Loss_content: 1.85405e+08, Loss_style: 4.33702e+06\n",
            "iteration:   47,  Loss_total: 4.36657e+06, Loss_content: 1.86683e+08, Loss_style: 4.17988e+06\n",
            "iteration:   48,  Loss_total: 4.22815e+06, Loss_content: 1.86948e+08, Loss_style: 4.0412e+06\n",
            "iteration:   49,  Loss_total: 4.15984e+06, Loss_content: 1.86628e+08, Loss_style: 3.97322e+06\n",
            "iteration:   50,  Loss_total: 4.10354e+06, Loss_content: 1.86983e+08, Loss_style: 3.91655e+06\n",
            "iteration:   51,  Loss_total: 4.00482e+06, Loss_content: 1.87332e+08, Loss_style: 3.81749e+06\n",
            "iteration:   52,  Loss_total: 3.89721e+06, Loss_content: 1.88384e+08, Loss_style: 3.70883e+06\n",
            "iteration:   53,  Loss_total: 3.78641e+06, Loss_content: 1.88543e+08, Loss_style: 3.59787e+06\n",
            "iteration:   54,  Loss_total: 3.72406e+06, Loss_content: 1.88236e+08, Loss_style: 3.53582e+06\n",
            "iteration:   55,  Loss_total: 3.65748e+06, Loss_content: 1.8864e+08, Loss_style: 3.46884e+06\n",
            "iteration:   56,  Loss_total: 3.59113e+06, Loss_content: 1.88529e+08, Loss_style: 3.4026e+06\n",
            "iteration:   57,  Loss_total: 3.49235e+06, Loss_content: 1.89228e+08, Loss_style: 3.30312e+06\n",
            "iteration:   58,  Loss_total: 3.43996e+06, Loss_content: 1.89594e+08, Loss_style: 3.25037e+06\n",
            "iteration:   59,  Loss_total: 3.39895e+06, Loss_content: 1.89178e+08, Loss_style: 3.20977e+06\n",
            "iteration:   60,  Loss_total: 3.34984e+06, Loss_content: 1.8956e+08, Loss_style: 3.16028e+06\n",
            "iteration:   61,  Loss_total: 3.27518e+06, Loss_content: 1.9036e+08, Loss_style: 3.08482e+06\n",
            "iteration:   62,  Loss_total: 3.21735e+06, Loss_content: 1.90569e+08, Loss_style: 3.02678e+06\n",
            "iteration:   63,  Loss_total: 3.19798e+06, Loss_content: 1.91281e+08, Loss_style: 3.0067e+06\n",
            "iteration:   64,  Loss_total: 3.10801e+06, Loss_content: 1.90826e+08, Loss_style: 2.91718e+06\n",
            "iteration:   65,  Loss_total: 3.08804e+06, Loss_content: 1.90855e+08, Loss_style: 2.89719e+06\n",
            "iteration:   66,  Loss_total: 3.03987e+06, Loss_content: 1.90741e+08, Loss_style: 2.84913e+06\n",
            "iteration:   67,  Loss_total: 3.06057e+06, Loss_content: 1.91707e+08, Loss_style: 2.86886e+06\n",
            "iteration:   68,  Loss_total: 3.00915e+06, Loss_content: 1.9114e+08, Loss_style: 2.81801e+06\n",
            "iteration:   69,  Loss_total: 2.95805e+06, Loss_content: 1.91171e+08, Loss_style: 2.76688e+06\n",
            "iteration:   70,  Loss_total: 2.91147e+06, Loss_content: 1.91128e+08, Loss_style: 2.72034e+06\n",
            "iteration:   71,  Loss_total: 2.87445e+06, Loss_content: 1.91243e+08, Loss_style: 2.68321e+06\n",
            "iteration:   72,  Loss_total: 2.84417e+06, Loss_content: 1.91286e+08, Loss_style: 2.65288e+06\n",
            "iteration:   73,  Loss_total: 2.80124e+06, Loss_content: 1.91318e+08, Loss_style: 2.60992e+06\n",
            "iteration:   74,  Loss_total: 2.76728e+06, Loss_content: 1.9157e+08, Loss_style: 2.57571e+06\n",
            "iteration:   75,  Loss_total: 2.7372e+06, Loss_content: 1.91322e+08, Loss_style: 2.54588e+06\n",
            "iteration:   76,  Loss_total: 2.69168e+06, Loss_content: 1.91653e+08, Loss_style: 2.50003e+06\n",
            "iteration:   77,  Loss_total: 2.66569e+06, Loss_content: 1.91769e+08, Loss_style: 2.47392e+06\n",
            "iteration:   78,  Loss_total: 2.62555e+06, Loss_content: 1.92223e+08, Loss_style: 2.43333e+06\n",
            "iteration:   79,  Loss_total: 2.58512e+06, Loss_content: 1.92027e+08, Loss_style: 2.3931e+06\n",
            "iteration:   80,  Loss_total: 2.55764e+06, Loss_content: 1.92086e+08, Loss_style: 2.36556e+06\n",
            "iteration:   81,  Loss_total: 2.52638e+06, Loss_content: 1.92264e+08, Loss_style: 2.33412e+06\n",
            "iteration:   82,  Loss_total: 2.49062e+06, Loss_content: 1.92373e+08, Loss_style: 2.29825e+06\n",
            "iteration:   83,  Loss_total: 2.45616e+06, Loss_content: 1.92691e+08, Loss_style: 2.26347e+06\n",
            "iteration:   84,  Loss_total: 2.42148e+06, Loss_content: 1.92627e+08, Loss_style: 2.22885e+06\n",
            "iteration:   85,  Loss_total: 2.40454e+06, Loss_content: 1.92817e+08, Loss_style: 2.21172e+06\n",
            "iteration:   86,  Loss_total: 2.38268e+06, Loss_content: 1.92808e+08, Loss_style: 2.18988e+06\n",
            "iteration:   87,  Loss_total: 2.35686e+06, Loss_content: 1.93169e+08, Loss_style: 2.16369e+06\n",
            "iteration:   88,  Loss_total: 2.3269e+06, Loss_content: 1.93356e+08, Loss_style: 2.13355e+06\n",
            "iteration:   89,  Loss_total: 2.31057e+06, Loss_content: 1.93284e+08, Loss_style: 2.11728e+06\n",
            "iteration:   90,  Loss_total: 2.29469e+06, Loss_content: 1.93406e+08, Loss_style: 2.10129e+06\n",
            "iteration:   91,  Loss_total: 2.26696e+06, Loss_content: 1.93493e+08, Loss_style: 2.07347e+06\n",
            "iteration:   92,  Loss_total: 2.24993e+06, Loss_content: 1.94107e+08, Loss_style: 2.05582e+06\n",
            "iteration:   93,  Loss_total: 2.2095e+06, Loss_content: 1.93843e+08, Loss_style: 2.01566e+06\n",
            "iteration:   94,  Loss_total: 2.20044e+06, Loss_content: 1.93907e+08, Loss_style: 2.00653e+06\n",
            "iteration:   95,  Loss_total: 2.17388e+06, Loss_content: 1.9413e+08, Loss_style: 1.97975e+06\n",
            "iteration:   96,  Loss_total: 2.14209e+06, Loss_content: 1.94497e+08, Loss_style: 1.94759e+06\n",
            "iteration:   97,  Loss_total: 2.13381e+06, Loss_content: 1.94586e+08, Loss_style: 1.93922e+06\n",
            "iteration:   98,  Loss_total: 2.107e+06, Loss_content: 1.94833e+08, Loss_style: 1.91217e+06\n",
            "iteration:   99,  Loss_total: 2.09105e+06, Loss_content: 1.94422e+08, Loss_style: 1.89663e+06\n",
            "iteration:  100,  Loss_total: 2.07716e+06, Loss_content: 1.9453e+08, Loss_style: 1.88263e+06\n",
            "iteration:  101,  Loss_total: 2.06107e+06, Loss_content: 1.9461e+08, Loss_style: 1.86646e+06\n",
            "iteration:  102,  Loss_total: 2.04034e+06, Loss_content: 1.94676e+08, Loss_style: 1.84566e+06\n",
            "iteration:  103,  Loss_total: 2.03107e+06, Loss_content: 1.94499e+08, Loss_style: 1.83657e+06\n",
            "iteration:  104,  Loss_total: 2.00796e+06, Loss_content: 1.9452e+08, Loss_style: 1.81344e+06\n",
            "iteration:  105,  Loss_total: 2.00004e+06, Loss_content: 1.94636e+08, Loss_style: 1.80541e+06\n",
            "iteration:  106,  Loss_total: 1.99109e+06, Loss_content: 1.94685e+08, Loss_style: 1.7964e+06\n",
            "iteration:  107,  Loss_total: 1.97054e+06, Loss_content: 1.94976e+08, Loss_style: 1.77556e+06\n",
            "iteration:  108,  Loss_total: 1.97511e+06, Loss_content: 1.9499e+08, Loss_style: 1.78012e+06\n",
            "iteration:  109,  Loss_total: 1.95811e+06, Loss_content: 1.94977e+08, Loss_style: 1.76313e+06\n",
            "iteration:  110,  Loss_total: 1.94355e+06, Loss_content: 1.95156e+08, Loss_style: 1.7484e+06\n",
            "iteration:  111,  Loss_total: 1.92715e+06, Loss_content: 1.95327e+08, Loss_style: 1.73182e+06\n",
            "iteration:  112,  Loss_total: 1.91464e+06, Loss_content: 1.9538e+08, Loss_style: 1.71926e+06\n",
            "iteration:  113,  Loss_total: 1.89789e+06, Loss_content: 1.95667e+08, Loss_style: 1.70222e+06\n",
            "iteration:  114,  Loss_total: 1.87743e+06, Loss_content: 1.95593e+08, Loss_style: 1.68184e+06\n",
            "iteration:  115,  Loss_total: 1.86156e+06, Loss_content: 1.95785e+08, Loss_style: 1.66578e+06\n",
            "iteration:  116,  Loss_total: 1.84393e+06, Loss_content: 1.95825e+08, Loss_style: 1.6481e+06\n",
            "iteration:  117,  Loss_total: 1.8281e+06, Loss_content: 1.95938e+08, Loss_style: 1.63216e+06\n",
            "iteration:  118,  Loss_total: 1.80823e+06, Loss_content: 1.96276e+08, Loss_style: 1.61196e+06\n",
            "iteration:  119,  Loss_total: 1.79568e+06, Loss_content: 1.95991e+08, Loss_style: 1.59969e+06\n",
            "iteration:  120,  Loss_total: 1.78723e+06, Loss_content: 1.96036e+08, Loss_style: 1.5912e+06\n",
            "iteration:  121,  Loss_total: 1.77239e+06, Loss_content: 1.96156e+08, Loss_style: 1.57623e+06\n",
            "iteration:  122,  Loss_total: 1.75962e+06, Loss_content: 1.96269e+08, Loss_style: 1.56335e+06\n",
            "iteration:  123,  Loss_total: 1.74463e+06, Loss_content: 1.96473e+08, Loss_style: 1.54816e+06\n",
            "iteration:  124,  Loss_total: 1.72978e+06, Loss_content: 1.96393e+08, Loss_style: 1.53339e+06\n",
            "iteration:  125,  Loss_total: 1.72389e+06, Loss_content: 1.9659e+08, Loss_style: 1.5273e+06\n",
            "iteration:  126,  Loss_total: 1.71175e+06, Loss_content: 1.96573e+08, Loss_style: 1.51518e+06\n",
            "iteration:  127,  Loss_total: 1.70604e+06, Loss_content: 1.96593e+08, Loss_style: 1.50944e+06\n",
            "iteration:  128,  Loss_total: 1.69644e+06, Loss_content: 1.96662e+08, Loss_style: 1.49978e+06\n",
            "iteration:  129,  Loss_total: 1.68468e+06, Loss_content: 1.96751e+08, Loss_style: 1.48793e+06\n",
            "iteration:  130,  Loss_total: 1.67052e+06, Loss_content: 1.96625e+08, Loss_style: 1.4739e+06\n",
            "iteration:  131,  Loss_total: 1.6579e+06, Loss_content: 1.97072e+08, Loss_style: 1.46083e+06\n",
            "iteration:  132,  Loss_total: 1.64991e+06, Loss_content: 1.96914e+08, Loss_style: 1.45299e+06\n",
            "iteration:  133,  Loss_total: 1.63962e+06, Loss_content: 1.96853e+08, Loss_style: 1.44277e+06\n",
            "iteration:  134,  Loss_total: 1.63154e+06, Loss_content: 1.96933e+08, Loss_style: 1.43461e+06\n",
            "iteration:  135,  Loss_total: 1.6204e+06, Loss_content: 1.97074e+08, Loss_style: 1.42333e+06\n",
            "iteration:  136,  Loss_total: 1.60929e+06, Loss_content: 1.97184e+08, Loss_style: 1.41211e+06\n",
            "iteration:  137,  Loss_total: 1.59518e+06, Loss_content: 1.97363e+08, Loss_style: 1.39781e+06\n",
            "iteration:  138,  Loss_total: 1.58267e+06, Loss_content: 1.97407e+08, Loss_style: 1.38526e+06\n",
            "iteration:  139,  Loss_total: 1.57169e+06, Loss_content: 1.97535e+08, Loss_style: 1.37416e+06\n",
            "iteration:  140,  Loss_total: 1.5603e+06, Loss_content: 1.97573e+08, Loss_style: 1.36273e+06\n",
            "iteration:  141,  Loss_total: 1.55824e+06, Loss_content: 1.97725e+08, Loss_style: 1.36052e+06\n",
            "iteration:  142,  Loss_total: 1.55311e+06, Loss_content: 1.97645e+08, Loss_style: 1.35546e+06\n",
            "iteration:  143,  Loss_total: 1.5412e+06, Loss_content: 1.97637e+08, Loss_style: 1.34357e+06\n",
            "iteration:  144,  Loss_total: 1.53254e+06, Loss_content: 1.97705e+08, Loss_style: 1.33484e+06\n",
            "iteration:  145,  Loss_total: 1.52368e+06, Loss_content: 1.97684e+08, Loss_style: 1.326e+06\n",
            "iteration:  146,  Loss_total: 1.52306e+06, Loss_content: 1.97772e+08, Loss_style: 1.32529e+06\n",
            "iteration:  147,  Loss_total: 1.51643e+06, Loss_content: 1.97721e+08, Loss_style: 1.31871e+06\n",
            "iteration:  148,  Loss_total: 1.5073e+06, Loss_content: 1.97736e+08, Loss_style: 1.30956e+06\n",
            "iteration:  149,  Loss_total: 1.49645e+06, Loss_content: 1.97891e+08, Loss_style: 1.29856e+06\n",
            "iteration:  150,  Loss_total: 1.5011e+06, Loss_content: 1.97562e+08, Loss_style: 1.30353e+06\n",
            "iteration:  151,  Loss_total: 1.49e+06, Loss_content: 1.97747e+08, Loss_style: 1.29225e+06\n",
            "iteration:  152,  Loss_total: 1.48303e+06, Loss_content: 1.97819e+08, Loss_style: 1.28521e+06\n",
            "iteration:  153,  Loss_total: 1.46964e+06, Loss_content: 1.97935e+08, Loss_style: 1.27171e+06\n",
            "iteration:  154,  Loss_total: 1.461e+06, Loss_content: 1.9802e+08, Loss_style: 1.26298e+06\n",
            "iteration:  155,  Loss_total: 1.45058e+06, Loss_content: 1.98036e+08, Loss_style: 1.25254e+06\n",
            "iteration:  156,  Loss_total: 1.4443e+06, Loss_content: 1.98149e+08, Loss_style: 1.24615e+06\n",
            "iteration:  157,  Loss_total: 1.43602e+06, Loss_content: 1.98134e+08, Loss_style: 1.23789e+06\n",
            "iteration:  158,  Loss_total: 1.42691e+06, Loss_content: 1.98209e+08, Loss_style: 1.2287e+06\n",
            "iteration:  159,  Loss_total: 1.41819e+06, Loss_content: 1.98294e+08, Loss_style: 1.2199e+06\n",
            "iteration:  160,  Loss_total: 1.40913e+06, Loss_content: 1.98381e+08, Loss_style: 1.21074e+06\n",
            "iteration:  161,  Loss_total: 1.40213e+06, Loss_content: 1.98479e+08, Loss_style: 1.20365e+06\n",
            "iteration:  162,  Loss_total: 1.39216e+06, Loss_content: 1.98395e+08, Loss_style: 1.19376e+06\n",
            "iteration:  163,  Loss_total: 1.38598e+06, Loss_content: 1.98413e+08, Loss_style: 1.18757e+06\n",
            "iteration:  164,  Loss_total: 1.37839e+06, Loss_content: 1.9836e+08, Loss_style: 1.18003e+06\n",
            "iteration:  165,  Loss_total: 1.37662e+06, Loss_content: 1.9855e+08, Loss_style: 1.17807e+06\n",
            "iteration:  166,  Loss_total: 1.37222e+06, Loss_content: 1.98451e+08, Loss_style: 1.17377e+06\n",
            "iteration:  167,  Loss_total: 1.36298e+06, Loss_content: 1.98413e+08, Loss_style: 1.16456e+06\n",
            "iteration:  168,  Loss_total: 1.35414e+06, Loss_content: 1.98489e+08, Loss_style: 1.15565e+06\n",
            "iteration:  169,  Loss_total: 1.34691e+06, Loss_content: 1.98441e+08, Loss_style: 1.14847e+06\n",
            "iteration:  170,  Loss_total: 1.34086e+06, Loss_content: 1.98416e+08, Loss_style: 1.14244e+06\n",
            "iteration:  171,  Loss_total: 1.32738e+06, Loss_content: 1.98601e+08, Loss_style: 1.12878e+06\n",
            "iteration:  172,  Loss_total: 1.31969e+06, Loss_content: 1.98596e+08, Loss_style: 1.1211e+06\n",
            "iteration:  173,  Loss_total: 1.31103e+06, Loss_content: 1.98619e+08, Loss_style: 1.11241e+06\n",
            "iteration:  174,  Loss_total: 1.30462e+06, Loss_content: 1.98724e+08, Loss_style: 1.1059e+06\n",
            "iteration:  175,  Loss_total: 1.29706e+06, Loss_content: 1.98709e+08, Loss_style: 1.09835e+06\n",
            "iteration:  176,  Loss_total: 1.28448e+06, Loss_content: 1.98894e+08, Loss_style: 1.08559e+06\n",
            "iteration:  177,  Loss_total: 1.27717e+06, Loss_content: 1.98942e+08, Loss_style: 1.07823e+06\n",
            "iteration:  178,  Loss_total: 1.27202e+06, Loss_content: 1.98995e+08, Loss_style: 1.07302e+06\n",
            "iteration:  179,  Loss_total: 1.26292e+06, Loss_content: 1.99132e+08, Loss_style: 1.06378e+06\n",
            "iteration:  180,  Loss_total: 1.25335e+06, Loss_content: 1.9915e+08, Loss_style: 1.0542e+06\n",
            "iteration:  181,  Loss_total: 1.24348e+06, Loss_content: 1.99304e+08, Loss_style: 1.04418e+06\n",
            "iteration:  182,  Loss_total: 1.23891e+06, Loss_content: 1.99249e+08, Loss_style: 1.03966e+06\n",
            "iteration:  183,  Loss_total: 1.23128e+06, Loss_content: 1.99286e+08, Loss_style: 1.03199e+06\n",
            "iteration:  184,  Loss_total: 1.2254e+06, Loss_content: 1.9927e+08, Loss_style: 1.02614e+06\n",
            "iteration:  185,  Loss_total: 1.21676e+06, Loss_content: 1.99369e+08, Loss_style: 1.01739e+06\n",
            "iteration:  186,  Loss_total: 1.21104e+06, Loss_content: 1.99316e+08, Loss_style: 1.01173e+06\n",
            "iteration:  187,  Loss_total: 1.20409e+06, Loss_content: 1.99472e+08, Loss_style: 1.00462e+06\n",
            "iteration:  188,  Loss_total: 1.19931e+06, Loss_content: 1.99536e+08, Loss_style: 999772\n",
            "iteration:  189,  Loss_total: 1.19455e+06, Loss_content: 1.99644e+08, Loss_style: 994904\n",
            "iteration:  190,  Loss_total: 1.18661e+06, Loss_content: 1.99732e+08, Loss_style: 986880\n",
            "iteration:  191,  Loss_total: 1.17884e+06, Loss_content: 1.99701e+08, Loss_style: 979144\n",
            "iteration:  192,  Loss_total: 1.17431e+06, Loss_content: 1.99692e+08, Loss_style: 974623\n",
            "iteration:  193,  Loss_total: 1.16735e+06, Loss_content: 1.99685e+08, Loss_style: 967665\n",
            "iteration:  194,  Loss_total: 1.16515e+06, Loss_content: 1.9972e+08, Loss_style: 965425\n",
            "iteration:  195,  Loss_total: 1.15523e+06, Loss_content: 1.99728e+08, Loss_style: 955505\n",
            "iteration:  196,  Loss_total: 1.15168e+06, Loss_content: 1.99727e+08, Loss_style: 951952\n",
            "iteration:  197,  Loss_total: 1.14669e+06, Loss_content: 1.99741e+08, Loss_style: 946947\n",
            "iteration:  198,  Loss_total: 1.13961e+06, Loss_content: 1.99654e+08, Loss_style: 939954\n",
            "iteration:  199,  Loss_total: 1.13912e+06, Loss_content: 1.99951e+08, Loss_style: 939167\n",
            "iteration:  200,  Loss_total: 1.13378e+06, Loss_content: 1.99779e+08, Loss_style: 934003\n",
            "iteration:  201,  Loss_total: 1.12777e+06, Loss_content: 1.99752e+08, Loss_style: 928016\n",
            "iteration:  202,  Loss_total: 1.12195e+06, Loss_content: 1.99737e+08, Loss_style: 922208\n",
            "iteration:  203,  Loss_total: 1.11584e+06, Loss_content: 1.99794e+08, Loss_style: 916051\n",
            "iteration:  204,  Loss_total: 1.10814e+06, Loss_content: 1.99826e+08, Loss_style: 908314\n",
            "iteration:  205,  Loss_total: 1.1007e+06, Loss_content: 1.99878e+08, Loss_style: 900821\n",
            "iteration:  206,  Loss_total: 1.09567e+06, Loss_content: 2.00019e+08, Loss_style: 895656\n",
            "iteration:  207,  Loss_total: 1.08847e+06, Loss_content: 2.00002e+08, Loss_style: 888469\n",
            "iteration:  208,  Loss_total: 1.08046e+06, Loss_content: 2.00059e+08, Loss_style: 880398\n",
            "iteration:  209,  Loss_total: 1.07398e+06, Loss_content: 2.00104e+08, Loss_style: 873875\n",
            "iteration:  210,  Loss_total: 1.06659e+06, Loss_content: 2.00173e+08, Loss_style: 866413\n",
            "iteration:  211,  Loss_total: 1.06127e+06, Loss_content: 2.00131e+08, Loss_style: 861135\n",
            "iteration:  212,  Loss_total: 1.0553e+06, Loss_content: 2.00189e+08, Loss_style: 855108\n",
            "iteration:  213,  Loss_total: 1.05031e+06, Loss_content: 2.00271e+08, Loss_style: 850043\n",
            "iteration:  214,  Loss_total: 1.04572e+06, Loss_content: 2.00183e+08, Loss_style: 845541\n",
            "iteration:  215,  Loss_total: 1.03958e+06, Loss_content: 2.00243e+08, Loss_style: 839338\n",
            "iteration:  216,  Loss_total: 1.03497e+06, Loss_content: 2.00279e+08, Loss_style: 834687\n",
            "iteration:  217,  Loss_total: 1.03127e+06, Loss_content: 2.00253e+08, Loss_style: 831013\n",
            "iteration:  218,  Loss_total: 1.02704e+06, Loss_content: 2.00399e+08, Loss_style: 826638\n",
            "iteration:  219,  Loss_total: 1.01982e+06, Loss_content: 2.00263e+08, Loss_style: 819558\n",
            "iteration:  220,  Loss_total: 1.01668e+06, Loss_content: 2.00266e+08, Loss_style: 816418\n",
            "iteration:  221,  Loss_total: 1.00938e+06, Loss_content: 2.00343e+08, Loss_style: 809032\n",
            "iteration:  222,  Loss_total: 1.00334e+06, Loss_content: 2.00354e+08, Loss_style: 802988\n",
            "iteration:  223,  Loss_total: 996999, Loss_content: 2.00416e+08, Loss_style: 796583\n",
            "iteration:  224,  Loss_total: 989827, Loss_content: 2.00473e+08, Loss_style: 789354\n",
            "iteration:  225,  Loss_total: 986090, Loss_content: 2.0054e+08, Loss_style: 785550\n",
            "iteration:  226,  Loss_total: 979947, Loss_content: 2.00607e+08, Loss_style: 779341\n",
            "iteration:  227,  Loss_total: 974376, Loss_content: 2.00613e+08, Loss_style: 773762\n",
            "iteration:  228,  Loss_total: 996023, Loss_content: 2.00878e+08, Loss_style: 795145\n",
            "iteration:  229,  Loss_total: 970966, Loss_content: 2.00678e+08, Loss_style: 770288\n",
            "iteration:  230,  Loss_total: 965058, Loss_content: 2.00668e+08, Loss_style: 764390\n",
            "iteration:  231,  Loss_total: 960761, Loss_content: 2.00615e+08, Loss_style: 760146\n",
            "iteration:  232,  Loss_total: 953233, Loss_content: 2.00615e+08, Loss_style: 752618\n",
            "iteration:  233,  Loss_total: 949832, Loss_content: 2.0035e+08, Loss_style: 749482\n",
            "iteration:  234,  Loss_total: 944847, Loss_content: 2.00485e+08, Loss_style: 744362\n",
            "iteration:  235,  Loss_total: 941658, Loss_content: 2.00552e+08, Loss_style: 741106\n",
            "iteration:  236,  Loss_total: 937921, Loss_content: 2.00609e+08, Loss_style: 737312\n",
            "iteration:  237,  Loss_total: 930834, Loss_content: 2.00648e+08, Loss_style: 730186\n",
            "iteration:  238,  Loss_total: 923236, Loss_content: 2.00623e+08, Loss_style: 722613\n",
            "iteration:  239,  Loss_total: 918394, Loss_content: 2.00552e+08, Loss_style: 717842\n",
            "iteration:  240,  Loss_total: 915052, Loss_content: 2.00505e+08, Loss_style: 714547\n",
            "iteration:  241,  Loss_total: 909601, Loss_content: 2.00538e+08, Loss_style: 709063\n",
            "iteration:  242,  Loss_total: 903900, Loss_content: 2.00508e+08, Loss_style: 703391\n",
            "iteration:  243,  Loss_total: 899506, Loss_content: 2.00592e+08, Loss_style: 698913\n",
            "iteration:  244,  Loss_total: 896102, Loss_content: 2.00629e+08, Loss_style: 695474\n",
            "iteration:  245,  Loss_total: 890359, Loss_content: 2.00598e+08, Loss_style: 689761\n",
            "iteration:  246,  Loss_total: 887121, Loss_content: 2.00597e+08, Loss_style: 686524\n",
            "iteration:  247,  Loss_total: 880042, Loss_content: 2.00543e+08, Loss_style: 679499\n",
            "iteration:  248,  Loss_total: 876918, Loss_content: 2.00486e+08, Loss_style: 676433\n",
            "iteration:  249,  Loss_total: 872972, Loss_content: 2.005e+08, Loss_style: 672471\n",
            "iteration:  250,  Loss_total: 868994, Loss_content: 2.00439e+08, Loss_style: 668555\n",
            "iteration:  251,  Loss_total: 865824, Loss_content: 2.00459e+08, Loss_style: 665365\n",
            "iteration:  252,  Loss_total: 860789, Loss_content: 2.00535e+08, Loss_style: 660254\n",
            "iteration:  253,  Loss_total: 857440, Loss_content: 2.00604e+08, Loss_style: 656836\n",
            "iteration:  254,  Loss_total: 852730, Loss_content: 2.00608e+08, Loss_style: 652121\n",
            "iteration:  255,  Loss_total: 847979, Loss_content: 2.00604e+08, Loss_style: 647375\n",
            "iteration:  256,  Loss_total: 844906, Loss_content: 2.00603e+08, Loss_style: 644303\n",
            "iteration:  257,  Loss_total: 839567, Loss_content: 2.00587e+08, Loss_style: 638980\n",
            "iteration:  258,  Loss_total: 832395, Loss_content: 2.00669e+08, Loss_style: 631726\n",
            "iteration:  259,  Loss_total: 829397, Loss_content: 2.00609e+08, Loss_style: 628788\n",
            "iteration:  260,  Loss_total: 825604, Loss_content: 2.00665e+08, Loss_style: 624939\n",
            "iteration:  261,  Loss_total: 822402, Loss_content: 2.00731e+08, Loss_style: 621671\n",
            "iteration:  262,  Loss_total: 816724, Loss_content: 2.00699e+08, Loss_style: 616025\n",
            "iteration:  263,  Loss_total: 811598, Loss_content: 2.00687e+08, Loss_style: 610910\n",
            "iteration:  264,  Loss_total: 808922, Loss_content: 2.00706e+08, Loss_style: 608215\n",
            "iteration:  265,  Loss_total: 800438, Loss_content: 2.00792e+08, Loss_style: 599645\n",
            "iteration:  266,  Loss_total: 796307, Loss_content: 2.00923e+08, Loss_style: 595383\n",
            "iteration:  267,  Loss_total: 790398, Loss_content: 2.00944e+08, Loss_style: 589454\n",
            "iteration:  268,  Loss_total: 791598, Loss_content: 2.00738e+08, Loss_style: 590860\n",
            "iteration:  269,  Loss_total: 787971, Loss_content: 2.00851e+08, Loss_style: 587120\n",
            "iteration:  270,  Loss_total: 783923, Loss_content: 2.00884e+08, Loss_style: 583039\n",
            "iteration:  271,  Loss_total: 779504, Loss_content: 2.0095e+08, Loss_style: 578554\n",
            "iteration:  272,  Loss_total: 774295, Loss_content: 2.00957e+08, Loss_style: 573338\n",
            "iteration:  273,  Loss_total: 770003, Loss_content: 2.00941e+08, Loss_style: 569062\n",
            "iteration:  274,  Loss_total: 764873, Loss_content: 2.01065e+08, Loss_style: 563808\n",
            "iteration:  275,  Loss_total: 761939, Loss_content: 2.01031e+08, Loss_style: 560908\n",
            "iteration:  276,  Loss_total: 759155, Loss_content: 2.0098e+08, Loss_style: 558175\n",
            "iteration:  277,  Loss_total: 755092, Loss_content: 2.01093e+08, Loss_style: 553999\n",
            "iteration:  278,  Loss_total: 751368, Loss_content: 2.01156e+08, Loss_style: 550212\n",
            "iteration:  279,  Loss_total: 747579, Loss_content: 2.01157e+08, Loss_style: 546421\n",
            "iteration:  280,  Loss_total: 744680, Loss_content: 2.01056e+08, Loss_style: 543624\n",
            "iteration:  281,  Loss_total: 739996, Loss_content: 2.01091e+08, Loss_style: 538905\n",
            "iteration:  282,  Loss_total: 737811, Loss_content: 2.01066e+08, Loss_style: 536745\n",
            "iteration:  283,  Loss_total: 734523, Loss_content: 2.01085e+08, Loss_style: 533437\n",
            "iteration:  284,  Loss_total: 731271, Loss_content: 2.01033e+08, Loss_style: 530238\n",
            "iteration:  285,  Loss_total: 725751, Loss_content: 2.01046e+08, Loss_style: 524704\n",
            "iteration:  286,  Loss_total: 722999, Loss_content: 2.01056e+08, Loss_style: 521943\n",
            "iteration:  287,  Loss_total: 719299, Loss_content: 2.01087e+08, Loss_style: 518212\n",
            "iteration:  288,  Loss_total: 715185, Loss_content: 2.01014e+08, Loss_style: 514171\n",
            "iteration:  289,  Loss_total: 710960, Loss_content: 2.01149e+08, Loss_style: 509811\n",
            "iteration:  290,  Loss_total: 709650, Loss_content: 2.01092e+08, Loss_style: 508558\n",
            "iteration:  291,  Loss_total: 704989, Loss_content: 2.01068e+08, Loss_style: 503920\n",
            "iteration:  292,  Loss_total: 702241, Loss_content: 2.01064e+08, Loss_style: 501176\n",
            "iteration:  293,  Loss_total: 697889, Loss_content: 2.01038e+08, Loss_style: 496851\n",
            "iteration:  294,  Loss_total: 696089, Loss_content: 2.01036e+08, Loss_style: 495053\n",
            "iteration:  295,  Loss_total: 693137, Loss_content: 2.01007e+08, Loss_style: 492130\n",
            "iteration:  296,  Loss_total: 691398, Loss_content: 2.01019e+08, Loss_style: 490379\n",
            "iteration:  297,  Loss_total: 688961, Loss_content: 2.01013e+08, Loss_style: 487948\n",
            "iteration:  298,  Loss_total: 684857, Loss_content: 2.01038e+08, Loss_style: 483819\n",
            "iteration:  299,  Loss_total: 683733, Loss_content: 2.01037e+08, Loss_style: 482696\n",
            "iteration:  300,  Loss_total: 678188, Loss_content: 2.01002e+08, Loss_style: 477186\n",
            "iteration:  301,  Loss_total: 676357, Loss_content: 2.01032e+08, Loss_style: 475325\n",
            "iteration:  302,  Loss_total: 673965, Loss_content: 2.01005e+08, Loss_style: 472960\n",
            "iteration:  303,  Loss_total: 669037, Loss_content: 2.00995e+08, Loss_style: 468043\n",
            "iteration:  304,  Loss_total: 665560, Loss_content: 2.00947e+08, Loss_style: 464613\n",
            "iteration:  305,  Loss_total: 661292, Loss_content: 2.00887e+08, Loss_style: 460405\n",
            "iteration:  306,  Loss_total: 659405, Loss_content: 2.00878e+08, Loss_style: 458527\n",
            "iteration:  307,  Loss_total: 656310, Loss_content: 2.00907e+08, Loss_style: 455403\n",
            "iteration:  308,  Loss_total: 653272, Loss_content: 2.0091e+08, Loss_style: 452362\n",
            "iteration:  309,  Loss_total: 650540, Loss_content: 2.0085e+08, Loss_style: 449690\n",
            "iteration:  310,  Loss_total: 647514, Loss_content: 2.00841e+08, Loss_style: 446673\n",
            "iteration:  311,  Loss_total: 646163, Loss_content: 2.00857e+08, Loss_style: 445306\n",
            "iteration:  312,  Loss_total: 643211, Loss_content: 2.00851e+08, Loss_style: 442360\n",
            "iteration:  313,  Loss_total: 639256, Loss_content: 2.00969e+08, Loss_style: 438288\n",
            "iteration:  314,  Loss_total: 635695, Loss_content: 2.00945e+08, Loss_style: 434750\n",
            "iteration:  315,  Loss_total: 633601, Loss_content: 2.00966e+08, Loss_style: 432635\n",
            "iteration:  316,  Loss_total: 631328, Loss_content: 2.01e+08, Loss_style: 430327\n",
            "iteration:  317,  Loss_total: 627802, Loss_content: 2.01014e+08, Loss_style: 426788\n",
            "iteration:  318,  Loss_total: 624875, Loss_content: 2.01026e+08, Loss_style: 423849\n",
            "iteration:  319,  Loss_total: 619212, Loss_content: 2.0101e+08, Loss_style: 418202\n",
            "iteration:  320,  Loss_total: 617544, Loss_content: 2.00975e+08, Loss_style: 416569\n",
            "iteration:  321,  Loss_total: 614635, Loss_content: 2.00958e+08, Loss_style: 413678\n",
            "iteration:  322,  Loss_total: 613358, Loss_content: 2.00785e+08, Loss_style: 412573\n",
            "iteration:  323,  Loss_total: 609003, Loss_content: 2.00875e+08, Loss_style: 408128\n",
            "iteration:  324,  Loss_total: 606040, Loss_content: 2.00932e+08, Loss_style: 405108\n",
            "iteration:  325,  Loss_total: 603955, Loss_content: 2.00949e+08, Loss_style: 403006\n",
            "iteration:  326,  Loss_total: 602543, Loss_content: 2.009e+08, Loss_style: 401644\n",
            "iteration:  327,  Loss_total: 599522, Loss_content: 2.00914e+08, Loss_style: 398608\n",
            "iteration:  328,  Loss_total: 596635, Loss_content: 2.00892e+08, Loss_style: 395743\n",
            "iteration:  329,  Loss_total: 593752, Loss_content: 2.00887e+08, Loss_style: 392865\n",
            "iteration:  330,  Loss_total: 590625, Loss_content: 2.00847e+08, Loss_style: 389778\n",
            "iteration:  331,  Loss_total: 588092, Loss_content: 2.0092e+08, Loss_style: 387173\n",
            "iteration:  332,  Loss_total: 586371, Loss_content: 2.00885e+08, Loss_style: 385486\n",
            "iteration:  333,  Loss_total: 584363, Loss_content: 2.00864e+08, Loss_style: 383499\n",
            "iteration:  334,  Loss_total: 581967, Loss_content: 2.00835e+08, Loss_style: 381132\n",
            "iteration:  335,  Loss_total: 578536, Loss_content: 2.00773e+08, Loss_style: 377762\n",
            "iteration:  336,  Loss_total: 575747, Loss_content: 2.00813e+08, Loss_style: 374934\n",
            "iteration:  337,  Loss_total: 574182, Loss_content: 2.00768e+08, Loss_style: 373414\n",
            "iteration:  338,  Loss_total: 572129, Loss_content: 2.00742e+08, Loss_style: 371387\n",
            "iteration:  339,  Loss_total: 569025, Loss_content: 2.00737e+08, Loss_style: 368288\n",
            "iteration:  340,  Loss_total: 566161, Loss_content: 2.00664e+08, Loss_style: 365498\n",
            "iteration:  341,  Loss_total: 564222, Loss_content: 2.00642e+08, Loss_style: 363580\n",
            "iteration:  342,  Loss_total: 561362, Loss_content: 2.00601e+08, Loss_style: 360761\n",
            "iteration:  343,  Loss_total: 560327, Loss_content: 2.007e+08, Loss_style: 359627\n",
            "iteration:  344,  Loss_total: 558098, Loss_content: 2.00667e+08, Loss_style: 357431\n",
            "iteration:  345,  Loss_total: 556383, Loss_content: 2.00646e+08, Loss_style: 355737\n",
            "iteration:  346,  Loss_total: 554427, Loss_content: 2.00681e+08, Loss_style: 353746\n",
            "iteration:  347,  Loss_total: 551439, Loss_content: 2.00774e+08, Loss_style: 350665\n",
            "iteration:  348,  Loss_total: 550088, Loss_content: 2.00758e+08, Loss_style: 349330\n",
            "iteration:  349,  Loss_total: 548137, Loss_content: 2.0076e+08, Loss_style: 347377\n",
            "iteration:  350,  Loss_total: 544754, Loss_content: 2.00718e+08, Loss_style: 344036\n",
            "iteration:  351,  Loss_total: 542667, Loss_content: 2.00647e+08, Loss_style: 342020\n",
            "iteration:  352,  Loss_total: 540410, Loss_content: 2.00631e+08, Loss_style: 339779\n",
            "iteration:  353,  Loss_total: 538818, Loss_content: 2.00496e+08, Loss_style: 338323\n",
            "iteration:  354,  Loss_total: 536746, Loss_content: 2.0055e+08, Loss_style: 336195\n",
            "iteration:  355,  Loss_total: 535376, Loss_content: 2.00586e+08, Loss_style: 334790\n",
            "iteration:  356,  Loss_total: 533426, Loss_content: 2.00614e+08, Loss_style: 332811\n",
            "iteration:  357,  Loss_total: 529980, Loss_content: 2.00636e+08, Loss_style: 329344\n",
            "iteration:  358,  Loss_total: 529421, Loss_content: 2.00555e+08, Loss_style: 328866\n",
            "iteration:  359,  Loss_total: 526132, Loss_content: 2.00558e+08, Loss_style: 325574\n",
            "iteration:  360,  Loss_total: 525181, Loss_content: 2.00544e+08, Loss_style: 324637\n",
            "iteration:  361,  Loss_total: 523330, Loss_content: 2.00467e+08, Loss_style: 322863\n",
            "iteration:  362,  Loss_total: 520628, Loss_content: 2.00456e+08, Loss_style: 320172\n",
            "iteration:  363,  Loss_total: 517930, Loss_content: 2.00448e+08, Loss_style: 317481\n",
            "iteration:  364,  Loss_total: 515684, Loss_content: 2.00449e+08, Loss_style: 315234\n",
            "iteration:  365,  Loss_total: 514062, Loss_content: 2.00483e+08, Loss_style: 313580\n",
            "iteration:  366,  Loss_total: 514225, Loss_content: 2.00594e+08, Loss_style: 313631\n",
            "iteration:  367,  Loss_total: 512123, Loss_content: 2.00534e+08, Loss_style: 311589\n",
            "iteration:  368,  Loss_total: 509759, Loss_content: 2.00534e+08, Loss_style: 309224\n",
            "iteration:  369,  Loss_total: 507909, Loss_content: 2.00511e+08, Loss_style: 307398\n",
            "iteration:  370,  Loss_total: 505555, Loss_content: 2.00461e+08, Loss_style: 305093\n",
            "iteration:  371,  Loss_total: 504890, Loss_content: 2.00482e+08, Loss_style: 304408\n",
            "iteration:  372,  Loss_total: 503832, Loss_content: 2.00466e+08, Loss_style: 303366\n",
            "iteration:  373,  Loss_total: 501569, Loss_content: 2.00334e+08, Loss_style: 301235\n",
            "iteration:  374,  Loss_total: 500785, Loss_content: 2.00315e+08, Loss_style: 300470\n",
            "iteration:  375,  Loss_total: 499900, Loss_content: 2.00319e+08, Loss_style: 299581\n",
            "iteration:  376,  Loss_total: 497888, Loss_content: 2.0034e+08, Loss_style: 297548\n",
            "iteration:  377,  Loss_total: 495814, Loss_content: 2.00314e+08, Loss_style: 295500\n",
            "iteration:  378,  Loss_total: 493503, Loss_content: 2.0029e+08, Loss_style: 293214\n",
            "iteration:  379,  Loss_total: 490911, Loss_content: 2.00253e+08, Loss_style: 290658\n",
            "iteration:  380,  Loss_total: 488933, Loss_content: 2.00194e+08, Loss_style: 288738\n",
            "iteration:  381,  Loss_total: 488361, Loss_content: 2.00192e+08, Loss_style: 288170\n",
            "iteration:  382,  Loss_total: 487588, Loss_content: 2.00187e+08, Loss_style: 287402\n",
            "iteration:  383,  Loss_total: 485810, Loss_content: 2.00158e+08, Loss_style: 285651\n",
            "iteration:  384,  Loss_total: 483700, Loss_content: 2.00133e+08, Loss_style: 283567\n",
            "iteration:  385,  Loss_total: 482310, Loss_content: 2.00121e+08, Loss_style: 282189\n",
            "iteration:  386,  Loss_total: 480880, Loss_content: 2.00081e+08, Loss_style: 280799\n",
            "iteration:  387,  Loss_total: 478652, Loss_content: 2.00076e+08, Loss_style: 278576\n",
            "iteration:  388,  Loss_total: 476723, Loss_content: 2.00011e+08, Loss_style: 276712\n",
            "iteration:  389,  Loss_total: 475288, Loss_content: 1.99985e+08, Loss_style: 275302\n",
            "iteration:  390,  Loss_total: 474349, Loss_content: 1.9998e+08, Loss_style: 274369\n",
            "iteration:  391,  Loss_total: 473115, Loss_content: 1.99907e+08, Loss_style: 273208\n",
            "iteration:  392,  Loss_total: 470857, Loss_content: 1.99935e+08, Loss_style: 270922\n",
            "iteration:  393,  Loss_total: 469661, Loss_content: 1.99924e+08, Loss_style: 269737\n",
            "iteration:  394,  Loss_total: 467871, Loss_content: 1.99891e+08, Loss_style: 267980\n",
            "iteration:  395,  Loss_total: 465856, Loss_content: 1.9979e+08, Loss_style: 266066\n",
            "iteration:  396,  Loss_total: 464008, Loss_content: 1.99744e+08, Loss_style: 264264\n",
            "iteration:  397,  Loss_total: 462949, Loss_content: 1.99731e+08, Loss_style: 263219\n",
            "iteration:  398,  Loss_total: 461032, Loss_content: 1.99642e+08, Loss_style: 261390\n",
            "iteration:  399,  Loss_total: 459666, Loss_content: 1.99646e+08, Loss_style: 260019\n",
            "iteration:  400,  Loss_total: 458243, Loss_content: 1.9964e+08, Loss_style: 258602\n",
            "iteration:  401,  Loss_total: 456695, Loss_content: 1.99588e+08, Loss_style: 257107\n",
            "iteration:  402,  Loss_total: 455577, Loss_content: 1.99593e+08, Loss_style: 255985\n",
            "iteration:  403,  Loss_total: 454321, Loss_content: 1.99588e+08, Loss_style: 254733\n",
            "iteration:  404,  Loss_total: 452731, Loss_content: 1.99566e+08, Loss_style: 253165\n",
            "iteration:  405,  Loss_total: 450936, Loss_content: 1.99566e+08, Loss_style: 251369\n",
            "iteration:  406,  Loss_total: 448687, Loss_content: 1.99458e+08, Loss_style: 249229\n",
            "iteration:  407,  Loss_total: 447342, Loss_content: 1.99411e+08, Loss_style: 247931\n",
            "iteration:  408,  Loss_total: 446202, Loss_content: 1.9933e+08, Loss_style: 246872\n",
            "iteration:  409,  Loss_total: 446132, Loss_content: 1.99225e+08, Loss_style: 246907\n",
            "iteration:  410,  Loss_total: 445144, Loss_content: 1.99276e+08, Loss_style: 245868\n",
            "iteration:  411,  Loss_total: 443613, Loss_content: 1.99256e+08, Loss_style: 244356\n",
            "iteration:  412,  Loss_total: 441587, Loss_content: 1.99199e+08, Loss_style: 242388\n",
            "iteration:  413,  Loss_total: 439929, Loss_content: 1.9911e+08, Loss_style: 240819\n",
            "iteration:  414,  Loss_total: 438122, Loss_content: 1.99056e+08, Loss_style: 239066\n",
            "iteration:  415,  Loss_total: 436286, Loss_content: 1.98996e+08, Loss_style: 237290\n",
            "iteration:  416,  Loss_total: 435184, Loss_content: 1.98999e+08, Loss_style: 236185\n",
            "iteration:  417,  Loss_total: 433908, Loss_content: 1.98992e+08, Loss_style: 234916\n",
            "iteration:  418,  Loss_total: 432884, Loss_content: 1.99062e+08, Loss_style: 233822\n",
            "iteration:  419,  Loss_total: 431789, Loss_content: 1.99034e+08, Loss_style: 232755\n",
            "iteration:  420,  Loss_total: 431016, Loss_content: 1.99006e+08, Loss_style: 232010\n",
            "iteration:  421,  Loss_total: 429711, Loss_content: 1.98948e+08, Loss_style: 230762\n",
            "iteration:  422,  Loss_total: 428074, Loss_content: 1.9886e+08, Loss_style: 229214\n",
            "iteration:  423,  Loss_total: 427472, Loss_content: 1.98785e+08, Loss_style: 228687\n",
            "iteration:  424,  Loss_total: 426025, Loss_content: 1.98805e+08, Loss_style: 227219\n",
            "iteration:  425,  Loss_total: 425144, Loss_content: 1.98812e+08, Loss_style: 226332\n",
            "iteration:  426,  Loss_total: 423962, Loss_content: 1.98784e+08, Loss_style: 225178\n",
            "iteration:  427,  Loss_total: 421655, Loss_content: 1.98706e+08, Loss_style: 222950\n",
            "iteration:  428,  Loss_total: 421971, Loss_content: 1.98557e+08, Loss_style: 223414\n",
            "iteration:  429,  Loss_total: 420480, Loss_content: 1.98632e+08, Loss_style: 221847\n",
            "iteration:  430,  Loss_total: 418900, Loss_content: 1.9859e+08, Loss_style: 220310\n",
            "iteration:  431,  Loss_total: 417536, Loss_content: 1.98514e+08, Loss_style: 219022\n",
            "iteration:  432,  Loss_total: 416757, Loss_content: 1.98508e+08, Loss_style: 218250\n",
            "iteration:  433,  Loss_total: 415933, Loss_content: 1.98494e+08, Loss_style: 217440\n",
            "iteration:  434,  Loss_total: 414912, Loss_content: 1.98402e+08, Loss_style: 216509\n",
            "iteration:  435,  Loss_total: 413622, Loss_content: 1.98415e+08, Loss_style: 215208\n",
            "iteration:  436,  Loss_total: 412639, Loss_content: 1.98391e+08, Loss_style: 214247\n",
            "iteration:  437,  Loss_total: 411781, Loss_content: 1.98355e+08, Loss_style: 213426\n",
            "iteration:  438,  Loss_total: 410764, Loss_content: 1.98283e+08, Loss_style: 212481\n",
            "iteration:  439,  Loss_total: 409401, Loss_content: 1.98218e+08, Loss_style: 211183\n",
            "iteration:  440,  Loss_total: 408210, Loss_content: 1.98142e+08, Loss_style: 210067\n",
            "iteration:  441,  Loss_total: 407041, Loss_content: 1.98095e+08, Loss_style: 208946\n",
            "iteration:  442,  Loss_total: 405882, Loss_content: 1.98042e+08, Loss_style: 207840\n",
            "iteration:  443,  Loss_total: 404725, Loss_content: 1.98012e+08, Loss_style: 206713\n",
            "iteration:  444,  Loss_total: 404114, Loss_content: 1.97934e+08, Loss_style: 206180\n",
            "iteration:  445,  Loss_total: 402671, Loss_content: 1.97922e+08, Loss_style: 204748\n",
            "iteration:  446,  Loss_total: 401689, Loss_content: 1.97925e+08, Loss_style: 203764\n",
            "iteration:  447,  Loss_total: 400669, Loss_content: 1.97872e+08, Loss_style: 202797\n",
            "iteration:  448,  Loss_total: 399687, Loss_content: 1.97841e+08, Loss_style: 201846\n",
            "iteration:  449,  Loss_total: 398574, Loss_content: 1.97725e+08, Loss_style: 200850\n",
            "iteration:  450,  Loss_total: 397474, Loss_content: 1.97736e+08, Loss_style: 199738\n",
            "iteration:  451,  Loss_total: 396922, Loss_content: 1.97743e+08, Loss_style: 199179\n",
            "iteration:  452,  Loss_total: 396173, Loss_content: 1.97712e+08, Loss_style: 198461\n",
            "iteration:  453,  Loss_total: 395105, Loss_content: 1.97647e+08, Loss_style: 197458\n",
            "iteration:  454,  Loss_total: 394022, Loss_content: 1.97557e+08, Loss_style: 196465\n",
            "iteration:  455,  Loss_total: 392737, Loss_content: 1.9748e+08, Loss_style: 195258\n",
            "iteration:  456,  Loss_total: 391808, Loss_content: 1.97426e+08, Loss_style: 194381\n",
            "iteration:  457,  Loss_total: 390427, Loss_content: 1.97351e+08, Loss_style: 193076\n",
            "iteration:  458,  Loss_total: 391145, Loss_content: 1.97293e+08, Loss_style: 193852\n",
            "iteration:  459,  Loss_total: 389606, Loss_content: 1.9732e+08, Loss_style: 192286\n",
            "iteration:  460,  Loss_total: 388313, Loss_content: 1.97277e+08, Loss_style: 191036\n",
            "iteration:  461,  Loss_total: 390964, Loss_content: 1.97126e+08, Loss_style: 193838\n",
            "iteration:  462,  Loss_total: 387766, Loss_content: 1.97226e+08, Loss_style: 190540\n",
            "iteration:  463,  Loss_total: 387026, Loss_content: 1.97206e+08, Loss_style: 189820\n",
            "iteration:  464,  Loss_total: 386055, Loss_content: 1.97178e+08, Loss_style: 188877\n",
            "iteration:  465,  Loss_total: 385575, Loss_content: 1.97073e+08, Loss_style: 188502\n",
            "iteration:  466,  Loss_total: 384292, Loss_content: 1.97084e+08, Loss_style: 187208\n",
            "iteration:  467,  Loss_total: 383365, Loss_content: 1.97049e+08, Loss_style: 186316\n",
            "iteration:  468,  Loss_total: 382738, Loss_content: 1.96999e+08, Loss_style: 185739\n",
            "iteration:  469,  Loss_total: 382201, Loss_content: 1.96943e+08, Loss_style: 185257\n",
            "iteration:  470,  Loss_total: 381065, Loss_content: 1.96896e+08, Loss_style: 184168\n",
            "iteration:  471,  Loss_total: 379967, Loss_content: 1.96821e+08, Loss_style: 183146\n",
            "iteration:  472,  Loss_total: 379240, Loss_content: 1.9676e+08, Loss_style: 182480\n",
            "iteration:  473,  Loss_total: 380751, Loss_content: 1.96704e+08, Loss_style: 184048\n",
            "iteration:  474,  Loss_total: 378677, Loss_content: 1.96739e+08, Loss_style: 181938\n",
            "iteration:  475,  Loss_total: 377782, Loss_content: 1.96641e+08, Loss_style: 181141\n",
            "iteration:  476,  Loss_total: 377143, Loss_content: 1.96589e+08, Loss_style: 180554\n",
            "iteration:  477,  Loss_total: 376188, Loss_content: 1.96539e+08, Loss_style: 179649\n",
            "iteration:  478,  Loss_total: 374864, Loss_content: 1.96445e+08, Loss_style: 178419\n",
            "iteration:  479,  Loss_total: 374393, Loss_content: 1.96357e+08, Loss_style: 178036\n",
            "iteration:  480,  Loss_total: 373561, Loss_content: 1.96364e+08, Loss_style: 177197\n",
            "iteration:  481,  Loss_total: 372896, Loss_content: 1.96323e+08, Loss_style: 176573\n",
            "iteration:  482,  Loss_total: 372198, Loss_content: 1.96278e+08, Loss_style: 175920\n",
            "iteration:  483,  Loss_total: 371452, Loss_content: 1.96134e+08, Loss_style: 175318\n",
            "iteration:  484,  Loss_total: 370276, Loss_content: 1.96109e+08, Loss_style: 174167\n",
            "iteration:  485,  Loss_total: 369617, Loss_content: 1.96056e+08, Loss_style: 173561\n",
            "iteration:  486,  Loss_total: 369153, Loss_content: 1.96009e+08, Loss_style: 173144\n",
            "iteration:  487,  Loss_total: 368549, Loss_content: 1.95953e+08, Loss_style: 172596\n",
            "iteration:  488,  Loss_total: 367574, Loss_content: 1.9587e+08, Loss_style: 171704\n",
            "iteration:  489,  Loss_total: 367106, Loss_content: 1.9572e+08, Loss_style: 171385\n",
            "iteration:  490,  Loss_total: 366065, Loss_content: 1.95735e+08, Loss_style: 170331\n",
            "iteration:  491,  Loss_total: 365350, Loss_content: 1.95711e+08, Loss_style: 169639\n",
            "iteration:  492,  Loss_total: 364795, Loss_content: 1.9567e+08, Loss_style: 169124\n",
            "iteration:  493,  Loss_total: 364054, Loss_content: 1.95602e+08, Loss_style: 168451\n",
            "iteration:  494,  Loss_total: 363357, Loss_content: 1.95486e+08, Loss_style: 167871\n",
            "iteration:  495,  Loss_total: 362331, Loss_content: 1.95425e+08, Loss_style: 166906\n",
            "iteration:  496,  Loss_total: 361592, Loss_content: 1.9537e+08, Loss_style: 166221\n",
            "iteration:  497,  Loss_total: 360688, Loss_content: 1.95264e+08, Loss_style: 165424\n",
            "iteration:  498,  Loss_total: 360187, Loss_content: 1.95212e+08, Loss_style: 164975\n",
            "iteration:  499,  Loss_total: 359613, Loss_content: 1.95195e+08, Loss_style: 164419\n",
            "iteration:  500,  Loss_total: 358636, Loss_content: 1.95113e+08, Loss_style: 163524\n",
            "iteration:  501,  Loss_total: 358120, Loss_content: 1.95096e+08, Loss_style: 163025\n",
            "iteration:  502,  Loss_total: 357389, Loss_content: 1.95004e+08, Loss_style: 162385\n",
            "iteration:  503,  Loss_total: 356720, Loss_content: 1.94975e+08, Loss_style: 161745\n",
            "iteration:  504,  Loss_total: 356014, Loss_content: 1.94929e+08, Loss_style: 161085\n",
            "iteration:  505,  Loss_total: 355492, Loss_content: 1.94917e+08, Loss_style: 160575\n",
            "iteration:  506,  Loss_total: 354641, Loss_content: 1.94868e+08, Loss_style: 159773\n",
            "iteration:  507,  Loss_total: 353812, Loss_content: 1.94774e+08, Loss_style: 159037\n",
            "iteration:  508,  Loss_total: 353148, Loss_content: 1.9472e+08, Loss_style: 158427\n",
            "iteration:  509,  Loss_total: 352167, Loss_content: 1.94599e+08, Loss_style: 157567\n",
            "iteration:  510,  Loss_total: 352328, Loss_content: 1.94515e+08, Loss_style: 157813\n",
            "iteration:  511,  Loss_total: 351710, Loss_content: 1.94557e+08, Loss_style: 157153\n",
            "iteration:  512,  Loss_total: 351231, Loss_content: 1.94551e+08, Loss_style: 156680\n",
            "iteration:  513,  Loss_total: 350424, Loss_content: 1.94493e+08, Loss_style: 155932\n",
            "iteration:  514,  Loss_total: 349608, Loss_content: 1.94387e+08, Loss_style: 155221\n",
            "iteration:  515,  Loss_total: 348663, Loss_content: 1.94311e+08, Loss_style: 154352\n",
            "iteration:  516,  Loss_total: 348001, Loss_content: 1.94222e+08, Loss_style: 153779\n",
            "iteration:  517,  Loss_total: 347513, Loss_content: 1.94203e+08, Loss_style: 153310\n",
            "iteration:  518,  Loss_total: 346869, Loss_content: 1.94128e+08, Loss_style: 152741\n",
            "iteration:  519,  Loss_total: 346046, Loss_content: 1.94087e+08, Loss_style: 151958\n",
            "iteration:  520,  Loss_total: 345412, Loss_content: 1.9408e+08, Loss_style: 151332\n",
            "iteration:  521,  Loss_total: 345078, Loss_content: 1.94055e+08, Loss_style: 151022\n",
            "iteration:  522,  Loss_total: 344533, Loss_content: 1.94012e+08, Loss_style: 150521\n",
            "iteration:  523,  Loss_total: 343857, Loss_content: 1.93921e+08, Loss_style: 149935\n",
            "iteration:  524,  Loss_total: 343243, Loss_content: 1.93855e+08, Loss_style: 149388\n",
            "iteration:  525,  Loss_total: 342650, Loss_content: 1.93809e+08, Loss_style: 148841\n",
            "iteration:  526,  Loss_total: 341954, Loss_content: 1.93739e+08, Loss_style: 148216\n",
            "iteration:  527,  Loss_total: 341294, Loss_content: 1.93628e+08, Loss_style: 147666\n",
            "iteration:  528,  Loss_total: 340570, Loss_content: 1.93564e+08, Loss_style: 147005\n",
            "iteration:  529,  Loss_total: 340170, Loss_content: 1.93449e+08, Loss_style: 146721\n",
            "iteration:  530,  Loss_total: 339457, Loss_content: 1.93477e+08, Loss_style: 145981\n",
            "iteration:  531,  Loss_total: 338968, Loss_content: 1.93464e+08, Loss_style: 145503\n",
            "iteration:  532,  Loss_total: 338499, Loss_content: 1.93421e+08, Loss_style: 145078\n",
            "iteration:  533,  Loss_total: 337931, Loss_content: 1.93349e+08, Loss_style: 144582\n",
            "iteration:  534,  Loss_total: 337331, Loss_content: 1.93284e+08, Loss_style: 144046\n",
            "iteration:  535,  Loss_total: 336630, Loss_content: 1.93199e+08, Loss_style: 143431\n",
            "iteration:  536,  Loss_total: 336131, Loss_content: 1.93108e+08, Loss_style: 143023\n",
            "iteration:  537,  Loss_total: 336021, Loss_content: 1.92956e+08, Loss_style: 143065\n",
            "iteration:  538,  Loss_total: 334855, Loss_content: 1.92977e+08, Loss_style: 141878\n",
            "iteration:  539,  Loss_total: 334412, Loss_content: 1.92981e+08, Loss_style: 141431\n",
            "iteration:  540,  Loss_total: 333963, Loss_content: 1.92979e+08, Loss_style: 140984\n",
            "iteration:  541,  Loss_total: 333352, Loss_content: 1.92887e+08, Loss_style: 140465\n",
            "iteration:  542,  Loss_total: 332630, Loss_content: 1.92801e+08, Loss_style: 139829\n",
            "iteration:  543,  Loss_total: 332118, Loss_content: 1.92652e+08, Loss_style: 139466\n",
            "iteration:  544,  Loss_total: 331353, Loss_content: 1.92615e+08, Loss_style: 138738\n",
            "iteration:  545,  Loss_total: 331090, Loss_content: 1.92603e+08, Loss_style: 138487\n",
            "iteration:  546,  Loss_total: 330591, Loss_content: 1.92598e+08, Loss_style: 137992\n",
            "iteration:  547,  Loss_total: 330070, Loss_content: 1.92543e+08, Loss_style: 137527\n",
            "iteration:  548,  Loss_total: 329636, Loss_content: 1.92495e+08, Loss_style: 137141\n",
            "iteration:  549,  Loss_total: 328994, Loss_content: 1.92425e+08, Loss_style: 136569\n",
            "iteration:  550,  Loss_total: 328440, Loss_content: 1.92328e+08, Loss_style: 136111\n",
            "iteration:  551,  Loss_total: 327863, Loss_content: 1.92228e+08, Loss_style: 135635\n",
            "iteration:  552,  Loss_total: 327478, Loss_content: 1.92217e+08, Loss_style: 135260\n",
            "iteration:  553,  Loss_total: 327162, Loss_content: 1.92187e+08, Loss_style: 134975\n",
            "iteration:  554,  Loss_total: 326598, Loss_content: 1.9215e+08, Loss_style: 134448\n",
            "iteration:  555,  Loss_total: 325963, Loss_content: 1.92093e+08, Loss_style: 133870\n",
            "iteration:  556,  Loss_total: 325421, Loss_content: 1.92004e+08, Loss_style: 133417\n",
            "iteration:  557,  Loss_total: 324849, Loss_content: 1.91983e+08, Loss_style: 132865\n",
            "iteration:  558,  Loss_total: 324404, Loss_content: 1.91961e+08, Loss_style: 132443\n",
            "iteration:  559,  Loss_total: 323927, Loss_content: 1.91917e+08, Loss_style: 132011\n",
            "iteration:  560,  Loss_total: 323429, Loss_content: 1.91818e+08, Loss_style: 131610\n",
            "iteration:  561,  Loss_total: 322556, Loss_content: 1.91763e+08, Loss_style: 130792\n",
            "iteration:  562,  Loss_total: 322138, Loss_content: 1.9177e+08, Loss_style: 130368\n",
            "iteration:  563,  Loss_total: 321781, Loss_content: 1.91747e+08, Loss_style: 130034\n",
            "iteration:  564,  Loss_total: 321318, Loss_content: 1.91692e+08, Loss_style: 129626\n",
            "iteration:  565,  Loss_total: 320688, Loss_content: 1.91622e+08, Loss_style: 129066\n",
            "iteration:  566,  Loss_total: 320210, Loss_content: 1.91546e+08, Loss_style: 128663\n",
            "iteration:  567,  Loss_total: 319780, Loss_content: 1.91517e+08, Loss_style: 128263\n",
            "iteration:  568,  Loss_total: 319389, Loss_content: 1.91497e+08, Loss_style: 127892\n",
            "iteration:  569,  Loss_total: 318778, Loss_content: 1.91433e+08, Loss_style: 127345\n",
            "iteration:  570,  Loss_total: 317972, Loss_content: 1.91279e+08, Loss_style: 126693\n",
            "iteration:  571,  Loss_total: 317473, Loss_content: 1.91171e+08, Loss_style: 126302\n",
            "iteration:  572,  Loss_total: 317058, Loss_content: 1.91136e+08, Loss_style: 125921\n",
            "iteration:  573,  Loss_total: 316631, Loss_content: 1.91077e+08, Loss_style: 125554\n",
            "iteration:  574,  Loss_total: 316225, Loss_content: 1.91023e+08, Loss_style: 125202\n",
            "iteration:  575,  Loss_total: 315768, Loss_content: 1.90897e+08, Loss_style: 124871\n",
            "iteration:  576,  Loss_total: 315270, Loss_content: 1.90895e+08, Loss_style: 124375\n",
            "iteration:  577,  Loss_total: 314919, Loss_content: 1.90874e+08, Loss_style: 124044\n",
            "iteration:  578,  Loss_total: 314451, Loss_content: 1.90811e+08, Loss_style: 123640\n",
            "iteration:  579,  Loss_total: 314049, Loss_content: 1.9072e+08, Loss_style: 123329\n",
            "iteration:  580,  Loss_total: 313467, Loss_content: 1.90688e+08, Loss_style: 122779\n",
            "iteration:  581,  Loss_total: 313156, Loss_content: 1.90651e+08, Loss_style: 122506\n",
            "iteration:  582,  Loss_total: 312599, Loss_content: 1.90589e+08, Loss_style: 122011\n",
            "iteration:  583,  Loss_total: 312658, Loss_content: 1.90362e+08, Loss_style: 122296\n",
            "iteration:  584,  Loss_total: 312085, Loss_content: 1.90471e+08, Loss_style: 121614\n",
            "iteration:  585,  Loss_total: 311400, Loss_content: 1.90424e+08, Loss_style: 120977\n",
            "iteration:  586,  Loss_total: 311053, Loss_content: 1.90265e+08, Loss_style: 120787\n",
            "iteration:  587,  Loss_total: 310461, Loss_content: 1.90292e+08, Loss_style: 120168\n",
            "iteration:  588,  Loss_total: 310090, Loss_content: 1.90255e+08, Loss_style: 119836\n",
            "iteration:  589,  Loss_total: 309518, Loss_content: 1.90158e+08, Loss_style: 119360\n",
            "iteration:  590,  Loss_total: 309055, Loss_content: 1.90041e+08, Loss_style: 119014\n",
            "iteration:  591,  Loss_total: 308519, Loss_content: 1.89988e+08, Loss_style: 118531\n",
            "iteration:  592,  Loss_total: 308060, Loss_content: 1.89913e+08, Loss_style: 118147\n",
            "iteration:  593,  Loss_total: 307644, Loss_content: 1.89852e+08, Loss_style: 117792\n",
            "iteration:  594,  Loss_total: 308370, Loss_content: 1.89664e+08, Loss_style: 118706\n",
            "iteration:  595,  Loss_total: 307339, Loss_content: 1.8978e+08, Loss_style: 117559\n",
            "iteration:  596,  Loss_total: 306780, Loss_content: 1.89726e+08, Loss_style: 117055\n",
            "iteration:  597,  Loss_total: 306399, Loss_content: 1.89657e+08, Loss_style: 116742\n",
            "iteration:  598,  Loss_total: 306061, Loss_content: 1.8968e+08, Loss_style: 116381\n",
            "iteration:  599,  Loss_total: 305842, Loss_content: 1.89587e+08, Loss_style: 116255\n",
            "iteration:  600,  Loss_total: 305443, Loss_content: 1.896e+08, Loss_style: 115843\n",
            "iteration:  601,  Loss_total: 304987, Loss_content: 1.89575e+08, Loss_style: 115412\n",
            "iteration:  602,  Loss_total: 304625, Loss_content: 1.89515e+08, Loss_style: 115109\n",
            "iteration:  603,  Loss_total: 303936, Loss_content: 1.89398e+08, Loss_style: 114537\n",
            "iteration:  604,  Loss_total: 304341, Loss_content: 1.8923e+08, Loss_style: 115111\n",
            "iteration:  605,  Loss_total: 303593, Loss_content: 1.89326e+08, Loss_style: 114267\n",
            "iteration:  606,  Loss_total: 303208, Loss_content: 1.8927e+08, Loss_style: 113938\n",
            "iteration:  607,  Loss_total: 302942, Loss_content: 1.89173e+08, Loss_style: 113769\n",
            "iteration:  608,  Loss_total: 302627, Loss_content: 1.89152e+08, Loss_style: 113475\n",
            "iteration:  609,  Loss_total: 302111, Loss_content: 1.89125e+08, Loss_style: 112986\n",
            "iteration:  610,  Loss_total: 301692, Loss_content: 1.89078e+08, Loss_style: 112613\n",
            "iteration:  611,  Loss_total: 301351, Loss_content: 1.89056e+08, Loss_style: 112295\n",
            "iteration:  612,  Loss_total: 300697, Loss_content: 1.88987e+08, Loss_style: 111710\n",
            "iteration:  613,  Loss_total: 300273, Loss_content: 1.88908e+08, Loss_style: 111366\n",
            "iteration:  614,  Loss_total: 299802, Loss_content: 1.88903e+08, Loss_style: 110899\n",
            "iteration:  615,  Loss_total: 299382, Loss_content: 1.88833e+08, Loss_style: 110549\n",
            "iteration:  616,  Loss_total: 299119, Loss_content: 1.88816e+08, Loss_style: 110303\n",
            "iteration:  617,  Loss_total: 298667, Loss_content: 1.88766e+08, Loss_style: 109901\n",
            "iteration:  618,  Loss_total: 298166, Loss_content: 1.88646e+08, Loss_style: 109519\n",
            "iteration:  619,  Loss_total: 297993, Loss_content: 1.88665e+08, Loss_style: 109328\n",
            "iteration:  620,  Loss_total: 297566, Loss_content: 1.88644e+08, Loss_style: 108923\n",
            "iteration:  621,  Loss_total: 297337, Loss_content: 1.88612e+08, Loss_style: 108725\n",
            "iteration:  622,  Loss_total: 297096, Loss_content: 1.88583e+08, Loss_style: 108514\n",
            "iteration:  623,  Loss_total: 296684, Loss_content: 1.88515e+08, Loss_style: 108168\n",
            "iteration:  624,  Loss_total: 296024, Loss_content: 1.88404e+08, Loss_style: 107620\n",
            "iteration:  625,  Loss_total: 295658, Loss_content: 1.88283e+08, Loss_style: 107375\n",
            "iteration:  626,  Loss_total: 295215, Loss_content: 1.88298e+08, Loss_style: 106917\n",
            "iteration:  627,  Loss_total: 294974, Loss_content: 1.88295e+08, Loss_style: 106679\n",
            "iteration:  628,  Loss_total: 294682, Loss_content: 1.88242e+08, Loss_style: 106439\n",
            "iteration:  629,  Loss_total: 294234, Loss_content: 1.88159e+08, Loss_style: 106075\n",
            "iteration:  630,  Loss_total: 293794, Loss_content: 1.88069e+08, Loss_style: 105725\n",
            "iteration:  631,  Loss_total: 293433, Loss_content: 1.87982e+08, Loss_style: 105451\n",
            "iteration:  632,  Loss_total: 293054, Loss_content: 1.8793e+08, Loss_style: 105124\n",
            "iteration:  633,  Loss_total: 292642, Loss_content: 1.87849e+08, Loss_style: 104793\n",
            "iteration:  634,  Loss_total: 292069, Loss_content: 1.87758e+08, Loss_style: 104311\n",
            "iteration:  635,  Loss_total: 291675, Loss_content: 1.87728e+08, Loss_style: 103947\n",
            "iteration:  636,  Loss_total: 291222, Loss_content: 1.87682e+08, Loss_style: 103540\n",
            "iteration:  637,  Loss_total: 290730, Loss_content: 1.87621e+08, Loss_style: 103109\n",
            "iteration:  638,  Loss_total: 290294, Loss_content: 1.87592e+08, Loss_style: 102702\n",
            "iteration:  639,  Loss_total: 290002, Loss_content: 1.87552e+08, Loss_style: 102450\n",
            "iteration:  640,  Loss_total: 289715, Loss_content: 1.87564e+08, Loss_style: 102151\n",
            "iteration:  641,  Loss_total: 289381, Loss_content: 1.87535e+08, Loss_style: 101846\n",
            "iteration:  642,  Loss_total: 289014, Loss_content: 1.87483e+08, Loss_style: 101532\n",
            "iteration:  643,  Loss_total: 288682, Loss_content: 1.87453e+08, Loss_style: 101229\n",
            "iteration:  644,  Loss_total: 288333, Loss_content: 1.87413e+08, Loss_style: 100920\n",
            "iteration:  645,  Loss_total: 287870, Loss_content: 1.87339e+08, Loss_style: 100531\n",
            "iteration:  646,  Loss_total: 287592, Loss_content: 1.87222e+08, Loss_style: 100370\n",
            "iteration:  647,  Loss_total: 287336, Loss_content: 1.87246e+08, Loss_style: 100091\n",
            "iteration:  648,  Loss_total: 287047, Loss_content: 1.87231e+08, Loss_style: 99816.9\n",
            "iteration:  649,  Loss_total: 286755, Loss_content: 1.87202e+08, Loss_style: 99553.2\n",
            "iteration:  650,  Loss_total: 286204, Loss_content: 1.87115e+08, Loss_style: 99089.4\n",
            "iteration:  651,  Loss_total: 286006, Loss_content: 1.87003e+08, Loss_style: 99003.2\n",
            "iteration:  652,  Loss_total: 285502, Loss_content: 1.87014e+08, Loss_style: 98487.2\n",
            "iteration:  653,  Loss_total: 285280, Loss_content: 1.86988e+08, Loss_style: 98292.3\n",
            "iteration:  654,  Loss_total: 284949, Loss_content: 1.86962e+08, Loss_style: 97986.7\n",
            "iteration:  655,  Loss_total: 284665, Loss_content: 1.86872e+08, Loss_style: 97793.2\n",
            "iteration:  656,  Loss_total: 284317, Loss_content: 1.86827e+08, Loss_style: 97489.8\n",
            "iteration:  657,  Loss_total: 284175, Loss_content: 1.86835e+08, Loss_style: 97340.8\n",
            "iteration:  658,  Loss_total: 283918, Loss_content: 1.86805e+08, Loss_style: 97113.2\n",
            "iteration:  659,  Loss_total: 283378, Loss_content: 1.86713e+08, Loss_style: 96665.3\n",
            "iteration:  660,  Loss_total: 283251, Loss_content: 1.86634e+08, Loss_style: 96616.4\n",
            "iteration:  661,  Loss_total: 282874, Loss_content: 1.86665e+08, Loss_style: 96209\n",
            "iteration:  662,  Loss_total: 282648, Loss_content: 1.8666e+08, Loss_style: 95988.8\n",
            "iteration:  663,  Loss_total: 282307, Loss_content: 1.86631e+08, Loss_style: 95675.8\n",
            "iteration:  664,  Loss_total: 281837, Loss_content: 1.86544e+08, Loss_style: 95293.1\n",
            "iteration:  665,  Loss_total: 281980, Loss_content: 1.8644e+08, Loss_style: 95540\n",
            "iteration:  666,  Loss_total: 281531, Loss_content: 1.86496e+08, Loss_style: 95035.4\n",
            "iteration:  667,  Loss_total: 281121, Loss_content: 1.86421e+08, Loss_style: 94699.3\n",
            "iteration:  668,  Loss_total: 280862, Loss_content: 1.86355e+08, Loss_style: 94507.3\n",
            "iteration:  669,  Loss_total: 280598, Loss_content: 1.8631e+08, Loss_style: 94288\n",
            "iteration:  670,  Loss_total: 280371, Loss_content: 1.86273e+08, Loss_style: 94098.1\n",
            "iteration:  671,  Loss_total: 280079, Loss_content: 1.86239e+08, Loss_style: 93839.3\n",
            "iteration:  672,  Loss_total: 279817, Loss_content: 1.86194e+08, Loss_style: 93623.1\n",
            "iteration:  673,  Loss_total: 279365, Loss_content: 1.86124e+08, Loss_style: 93240.8\n",
            "iteration:  674,  Loss_total: 279425, Loss_content: 1.85975e+08, Loss_style: 93450\n",
            "iteration:  675,  Loss_total: 279115, Loss_content: 1.8605e+08, Loss_style: 93065.9\n",
            "iteration:  676,  Loss_total: 278766, Loss_content: 1.86022e+08, Loss_style: 92743.4\n",
            "iteration:  677,  Loss_total: 278418, Loss_content: 1.85943e+08, Loss_style: 92476\n",
            "iteration:  678,  Loss_total: 278097, Loss_content: 1.85913e+08, Loss_style: 92183.6\n",
            "iteration:  679,  Loss_total: 277633, Loss_content: 1.85822e+08, Loss_style: 91811.3\n",
            "iteration:  680,  Loss_total: 277419, Loss_content: 1.85774e+08, Loss_style: 91645.2\n",
            "iteration:  681,  Loss_total: 277150, Loss_content: 1.85763e+08, Loss_style: 91387.4\n",
            "iteration:  682,  Loss_total: 276908, Loss_content: 1.85743e+08, Loss_style: 91164.7\n",
            "iteration:  683,  Loss_total: 276704, Loss_content: 1.85666e+08, Loss_style: 91037.6\n",
            "iteration:  684,  Loss_total: 276360, Loss_content: 1.85631e+08, Loss_style: 90728.9\n",
            "iteration:  685,  Loss_total: 276042, Loss_content: 1.85592e+08, Loss_style: 90450.7\n",
            "iteration:  686,  Loss_total: 275790, Loss_content: 1.8555e+08, Loss_style: 90240.8\n",
            "iteration:  687,  Loss_total: 275594, Loss_content: 1.85529e+08, Loss_style: 90064.9\n",
            "iteration:  688,  Loss_total: 275302, Loss_content: 1.8548e+08, Loss_style: 89822.7\n",
            "iteration:  689,  Loss_total: 274938, Loss_content: 1.85448e+08, Loss_style: 89490\n",
            "iteration:  690,  Loss_total: 274653, Loss_content: 1.85358e+08, Loss_style: 89295.4\n",
            "iteration:  691,  Loss_total: 274395, Loss_content: 1.85377e+08, Loss_style: 89017.7\n",
            "iteration:  692,  Loss_total: 274150, Loss_content: 1.85362e+08, Loss_style: 88787.7\n",
            "iteration:  693,  Loss_total: 273920, Loss_content: 1.85339e+08, Loss_style: 88580.2\n",
            "iteration:  694,  Loss_total: 273505, Loss_content: 1.85266e+08, Loss_style: 88238.1\n",
            "iteration:  695,  Loss_total: 273262, Loss_content: 1.85088e+08, Loss_style: 88174.3\n",
            "iteration:  696,  Loss_total: 272607, Loss_content: 1.85059e+08, Loss_style: 87547.9\n",
            "iteration:  697,  Loss_total: 272404, Loss_content: 1.85079e+08, Loss_style: 87324.6\n",
            "iteration:  698,  Loss_total: 272129, Loss_content: 1.85042e+08, Loss_style: 87087.2\n",
            "iteration:  699,  Loss_total: 271856, Loss_content: 1.84991e+08, Loss_style: 86865.1\n",
            "iteration:  700,  Loss_total: 271543, Loss_content: 1.84939e+08, Loss_style: 86604.4\n",
            "iteration:  701,  Loss_total: 271223, Loss_content: 1.84898e+08, Loss_style: 86324.9\n",
            "iteration:  702,  Loss_total: 270946, Loss_content: 1.84826e+08, Loss_style: 86120.5\n",
            "iteration:  703,  Loss_total: 270595, Loss_content: 1.84785e+08, Loss_style: 85809.4\n",
            "iteration:  704,  Loss_total: 270206, Loss_content: 1.84727e+08, Loss_style: 85479.1\n",
            "iteration:  705,  Loss_total: 269922, Loss_content: 1.84656e+08, Loss_style: 85266.4\n",
            "iteration:  706,  Loss_total: 269696, Loss_content: 1.84612e+08, Loss_style: 85083.6\n",
            "iteration:  707,  Loss_total: 269554, Loss_content: 1.84583e+08, Loss_style: 84970.3\n",
            "iteration:  708,  Loss_total: 269262, Loss_content: 1.84526e+08, Loss_style: 84735.9\n",
            "iteration:  709,  Loss_total: 269018, Loss_content: 1.84528e+08, Loss_style: 84489.5\n",
            "iteration:  710,  Loss_total: 268636, Loss_content: 1.84501e+08, Loss_style: 84135.4\n",
            "iteration:  711,  Loss_total: 268280, Loss_content: 1.84446e+08, Loss_style: 83834.1\n",
            "iteration:  712,  Loss_total: 267982, Loss_content: 1.84359e+08, Loss_style: 83622.7\n",
            "iteration:  713,  Loss_total: 267768, Loss_content: 1.8436e+08, Loss_style: 83407.9\n",
            "iteration:  714,  Loss_total: 267620, Loss_content: 1.84332e+08, Loss_style: 83287.2\n",
            "iteration:  715,  Loss_total: 267412, Loss_content: 1.8433e+08, Loss_style: 83081.1\n",
            "iteration:  716,  Loss_total: 266993, Loss_content: 1.84271e+08, Loss_style: 82722.9\n",
            "iteration:  717,  Loss_total: 266721, Loss_content: 1.84209e+08, Loss_style: 82512.6\n",
            "iteration:  718,  Loss_total: 266413, Loss_content: 1.84183e+08, Loss_style: 82230.5\n",
            "iteration:  719,  Loss_total: 266235, Loss_content: 1.84156e+08, Loss_style: 82079.1\n",
            "iteration:  720,  Loss_total: 265930, Loss_content: 1.84099e+08, Loss_style: 81830.9\n",
            "iteration:  721,  Loss_total: 265543, Loss_content: 1.84025e+08, Loss_style: 81518\n",
            "iteration:  722,  Loss_total: 265135, Loss_content: 1.83932e+08, Loss_style: 81202.9\n",
            "iteration:  723,  Loss_total: 264829, Loss_content: 1.83903e+08, Loss_style: 80926.3\n",
            "iteration:  724,  Loss_total: 264524, Loss_content: 1.83852e+08, Loss_style: 80672.2\n",
            "iteration:  725,  Loss_total: 264381, Loss_content: 1.83849e+08, Loss_style: 80531.7\n",
            "iteration:  726,  Loss_total: 264216, Loss_content: 1.83827e+08, Loss_style: 80388.6\n",
            "iteration:  727,  Loss_total: 264000, Loss_content: 1.83803e+08, Loss_style: 80197.1\n",
            "iteration:  728,  Loss_total: 263849, Loss_content: 1.83768e+08, Loss_style: 80080.4\n",
            "iteration:  729,  Loss_total: 263531, Loss_content: 1.83692e+08, Loss_style: 79839.1\n",
            "iteration:  730,  Loss_total: 263221, Loss_content: 1.8364e+08, Loss_style: 79581.3\n",
            "iteration:  731,  Loss_total: 262936, Loss_content: 1.83603e+08, Loss_style: 79332.2\n",
            "iteration:  732,  Loss_total: 262594, Loss_content: 1.83531e+08, Loss_style: 79062.8\n",
            "iteration:  733,  Loss_total: 262341, Loss_content: 1.83493e+08, Loss_style: 78848\n",
            "iteration:  734,  Loss_total: 262120, Loss_content: 1.83485e+08, Loss_style: 78635.1\n",
            "iteration:  735,  Loss_total: 261977, Loss_content: 1.83404e+08, Loss_style: 78573.1\n",
            "iteration:  736,  Loss_total: 261608, Loss_content: 1.83433e+08, Loss_style: 78175.2\n",
            "iteration:  737,  Loss_total: 261415, Loss_content: 1.8342e+08, Loss_style: 77994.5\n",
            "iteration:  738,  Loss_total: 261242, Loss_content: 1.83392e+08, Loss_style: 77850.1\n",
            "iteration:  739,  Loss_total: 260971, Loss_content: 1.83305e+08, Loss_style: 77666.1\n",
            "iteration:  740,  Loss_total: 260730, Loss_content: 1.83291e+08, Loss_style: 77438.8\n",
            "iteration:  741,  Loss_total: 260417, Loss_content: 1.83248e+08, Loss_style: 77169.6\n",
            "iteration:  742,  Loss_total: 260222, Loss_content: 1.83167e+08, Loss_style: 77055.1\n",
            "iteration:  743,  Loss_total: 260093, Loss_content: 1.83166e+08, Loss_style: 76927.2\n",
            "iteration:  744,  Loss_total: 259730, Loss_content: 1.83114e+08, Loss_style: 76616\n",
            "iteration:  745,  Loss_total: 259555, Loss_content: 1.83084e+08, Loss_style: 76470.2\n",
            "iteration:  746,  Loss_total: 259302, Loss_content: 1.83036e+08, Loss_style: 76266.8\n",
            "iteration:  747,  Loss_total: 259072, Loss_content: 1.82983e+08, Loss_style: 76089.2\n",
            "iteration:  748,  Loss_total: 258835, Loss_content: 1.82947e+08, Loss_style: 75888.4\n",
            "iteration:  749,  Loss_total: 258583, Loss_content: 1.82874e+08, Loss_style: 75709.4\n",
            "iteration:  750,  Loss_total: 258291, Loss_content: 1.8288e+08, Loss_style: 75411.7\n",
            "iteration:  751,  Loss_total: 258061, Loss_content: 1.82845e+08, Loss_style: 75216.3\n",
            "iteration:  752,  Loss_total: 257803, Loss_content: 1.82822e+08, Loss_style: 74980.9\n",
            "iteration:  753,  Loss_total: 257638, Loss_content: 1.82796e+08, Loss_style: 74841.4\n",
            "iteration:  754,  Loss_total: 257445, Loss_content: 1.82767e+08, Loss_style: 74678.4\n",
            "iteration:  755,  Loss_total: 257197, Loss_content: 1.82715e+08, Loss_style: 74481.3\n",
            "iteration:  756,  Loss_total: 256911, Loss_content: 1.82656e+08, Loss_style: 74255.6\n",
            "iteration:  757,  Loss_total: 256747, Loss_content: 1.82647e+08, Loss_style: 74099.6\n",
            "iteration:  758,  Loss_total: 256494, Loss_content: 1.82595e+08, Loss_style: 73899.6\n",
            "iteration:  759,  Loss_total: 256218, Loss_content: 1.82578e+08, Loss_style: 73640.2\n",
            "iteration:  760,  Loss_total: 256069, Loss_content: 1.82495e+08, Loss_style: 73574.5\n",
            "iteration:  761,  Loss_total: 255769, Loss_content: 1.82516e+08, Loss_style: 73253.8\n",
            "iteration:  762,  Loss_total: 255656, Loss_content: 1.82532e+08, Loss_style: 73124.2\n",
            "iteration:  763,  Loss_total: 255358, Loss_content: 1.82519e+08, Loss_style: 72838.7\n",
            "iteration:  764,  Loss_total: 255100, Loss_content: 1.82467e+08, Loss_style: 72632.1\n",
            "iteration:  765,  Loss_total: 254784, Loss_content: 1.82415e+08, Loss_style: 72369.1\n",
            "iteration:  766,  Loss_total: 254639, Loss_content: 1.82402e+08, Loss_style: 72236.6\n",
            "iteration:  767,  Loss_total: 254384, Loss_content: 1.82338e+08, Loss_style: 72046.5\n",
            "iteration:  768,  Loss_total: 254669, Loss_content: 1.82298e+08, Loss_style: 72371.3\n",
            "iteration:  769,  Loss_total: 254252, Loss_content: 1.82323e+08, Loss_style: 71929\n",
            "iteration:  770,  Loss_total: 254004, Loss_content: 1.82274e+08, Loss_style: 71729.9\n",
            "iteration:  771,  Loss_total: 253672, Loss_content: 1.82197e+08, Loss_style: 71475.2\n",
            "iteration:  772,  Loss_total: 253456, Loss_content: 1.82165e+08, Loss_style: 71291.3\n",
            "iteration:  773,  Loss_total: 253196, Loss_content: 1.82117e+08, Loss_style: 71079.2\n",
            "iteration:  774,  Loss_total: 252905, Loss_content: 1.82094e+08, Loss_style: 70811.2\n",
            "iteration:  775,  Loss_total: 252657, Loss_content: 1.82085e+08, Loss_style: 70572\n",
            "iteration:  776,  Loss_total: 252432, Loss_content: 1.82041e+08, Loss_style: 70391.1\n",
            "iteration:  777,  Loss_total: 252155, Loss_content: 1.82018e+08, Loss_style: 70136.9\n",
            "iteration:  778,  Loss_total: 251848, Loss_content: 1.81973e+08, Loss_style: 69874.8\n",
            "iteration:  779,  Loss_total: 251615, Loss_content: 1.81936e+08, Loss_style: 69678.5\n",
            "iteration:  780,  Loss_total: 251358, Loss_content: 1.81903e+08, Loss_style: 69455.1\n",
            "iteration:  781,  Loss_total: 251231, Loss_content: 1.81842e+08, Loss_style: 69388.8\n",
            "iteration:  782,  Loss_total: 250944, Loss_content: 1.81826e+08, Loss_style: 69117.2\n",
            "iteration:  783,  Loss_total: 250759, Loss_content: 1.81798e+08, Loss_style: 68960.4\n",
            "iteration:  784,  Loss_total: 250555, Loss_content: 1.81796e+08, Loss_style: 68758.9\n",
            "iteration:  785,  Loss_total: 250305, Loss_content: 1.81763e+08, Loss_style: 68542.1\n",
            "iteration:  786,  Loss_total: 250099, Loss_content: 1.81727e+08, Loss_style: 68371.9\n",
            "iteration:  787,  Loss_total: 249810, Loss_content: 1.81648e+08, Loss_style: 68162.1\n",
            "iteration:  788,  Loss_total: 249426, Loss_content: 1.81601e+08, Loss_style: 67824.9\n",
            "iteration:  789,  Loss_total: 249227, Loss_content: 1.81545e+08, Loss_style: 67682.1\n",
            "iteration:  790,  Loss_total: 248989, Loss_content: 1.81522e+08, Loss_style: 67467.1\n",
            "iteration:  791,  Loss_total: 248808, Loss_content: 1.81494e+08, Loss_style: 67313.3\n",
            "iteration:  792,  Loss_total: 248608, Loss_content: 1.81468e+08, Loss_style: 67139.9\n",
            "iteration:  793,  Loss_total: 248414, Loss_content: 1.81441e+08, Loss_style: 66973\n",
            "iteration:  794,  Loss_total: 248218, Loss_content: 1.8138e+08, Loss_style: 66838.2\n",
            "iteration:  795,  Loss_total: 248111, Loss_content: 1.81291e+08, Loss_style: 66820.7\n",
            "iteration:  796,  Loss_total: 247812, Loss_content: 1.81279e+08, Loss_style: 66532.8\n",
            "iteration:  797,  Loss_total: 247637, Loss_content: 1.81256e+08, Loss_style: 66381\n",
            "iteration:  798,  Loss_total: 247444, Loss_content: 1.81231e+08, Loss_style: 66212.8\n",
            "iteration:  799,  Loss_total: 247194, Loss_content: 1.81171e+08, Loss_style: 66022.8\n",
            "iteration:  800,  Loss_total: 246995, Loss_content: 1.8111e+08, Loss_style: 65885\n",
            "iteration:  801,  Loss_total: 246825, Loss_content: 1.81107e+08, Loss_style: 65718.9\n",
            "iteration:  802,  Loss_total: 246627, Loss_content: 1.81069e+08, Loss_style: 65558.5\n",
            "iteration:  803,  Loss_total: 246364, Loss_content: 1.81037e+08, Loss_style: 65326.4\n",
            "iteration:  804,  Loss_total: 246245, Loss_content: 1.80984e+08, Loss_style: 65260.2\n",
            "iteration:  805,  Loss_total: 246026, Loss_content: 1.80981e+08, Loss_style: 65044.6\n",
            "iteration:  806,  Loss_total: 245903, Loss_content: 1.80995e+08, Loss_style: 64908.3\n",
            "iteration:  807,  Loss_total: 245729, Loss_content: 1.80967e+08, Loss_style: 64761.8\n",
            "iteration:  808,  Loss_total: 245496, Loss_content: 1.80908e+08, Loss_style: 64587.3\n",
            "iteration:  809,  Loss_total: 245224, Loss_content: 1.80874e+08, Loss_style: 64350.7\n",
            "iteration:  810,  Loss_total: 245051, Loss_content: 1.80848e+08, Loss_style: 64202.8\n",
            "iteration:  811,  Loss_total: 244839, Loss_content: 1.80785e+08, Loss_style: 64053.9\n",
            "iteration:  812,  Loss_total: 244605, Loss_content: 1.80733e+08, Loss_style: 63871.7\n",
            "iteration:  813,  Loss_total: 244345, Loss_content: 1.80658e+08, Loss_style: 63687.2\n",
            "iteration:  814,  Loss_total: 244172, Loss_content: 1.80643e+08, Loss_style: 63528.7\n",
            "iteration:  815,  Loss_total: 243979, Loss_content: 1.80632e+08, Loss_style: 63347\n",
            "iteration:  816,  Loss_total: 243859, Loss_content: 1.80555e+08, Loss_style: 63303.2\n",
            "iteration:  817,  Loss_total: 243639, Loss_content: 1.80563e+08, Loss_style: 63076.2\n",
            "iteration:  818,  Loss_total: 243469, Loss_content: 1.80535e+08, Loss_style: 62934.1\n",
            "iteration:  819,  Loss_total: 243241, Loss_content: 1.80472e+08, Loss_style: 62769.2\n",
            "iteration:  820,  Loss_total: 243562, Loss_content: 1.80338e+08, Loss_style: 63223.7\n",
            "iteration:  821,  Loss_total: 243068, Loss_content: 1.80422e+08, Loss_style: 62646.4\n",
            "iteration:  822,  Loss_total: 242863, Loss_content: 1.80362e+08, Loss_style: 62500.8\n",
            "iteration:  823,  Loss_total: 242700, Loss_content: 1.80345e+08, Loss_style: 62354.2\n",
            "iteration:  824,  Loss_total: 242538, Loss_content: 1.80331e+08, Loss_style: 62207.3\n",
            "iteration:  825,  Loss_total: 242329, Loss_content: 1.80325e+08, Loss_style: 62003.3\n",
            "iteration:  826,  Loss_total: 242122, Loss_content: 1.8029e+08, Loss_style: 61832.5\n",
            "iteration:  827,  Loss_total: 241895, Loss_content: 1.80273e+08, Loss_style: 61621.8\n",
            "iteration:  828,  Loss_total: 241690, Loss_content: 1.80243e+08, Loss_style: 61447.6\n",
            "iteration:  829,  Loss_total: 241492, Loss_content: 1.80174e+08, Loss_style: 61318\n",
            "iteration:  830,  Loss_total: 241333, Loss_content: 1.80154e+08, Loss_style: 61179.2\n",
            "iteration:  831,  Loss_total: 241202, Loss_content: 1.80146e+08, Loss_style: 61055.9\n",
            "iteration:  832,  Loss_total: 241124, Loss_content: 1.80083e+08, Loss_style: 61041.2\n",
            "iteration:  833,  Loss_total: 240855, Loss_content: 1.80087e+08, Loss_style: 60768.2\n",
            "iteration:  834,  Loss_total: 240691, Loss_content: 1.80076e+08, Loss_style: 60614.9\n",
            "iteration:  835,  Loss_total: 240530, Loss_content: 1.80038e+08, Loss_style: 60492.1\n",
            "iteration:  836,  Loss_total: 240315, Loss_content: 1.79977e+08, Loss_style: 60337.7\n",
            "iteration:  837,  Loss_total: 240086, Loss_content: 1.79926e+08, Loss_style: 60160\n",
            "iteration:  838,  Loss_total: 239858, Loss_content: 1.79893e+08, Loss_style: 59964.3\n",
            "iteration:  839,  Loss_total: 239705, Loss_content: 1.79888e+08, Loss_style: 59817.2\n",
            "iteration:  840,  Loss_total: 239473, Loss_content: 1.79859e+08, Loss_style: 59614\n",
            "iteration:  841,  Loss_total: 239251, Loss_content: 1.7981e+08, Loss_style: 59441.5\n",
            "iteration:  842,  Loss_total: 239045, Loss_content: 1.7979e+08, Loss_style: 59254.4\n",
            "iteration:  843,  Loss_total: 238866, Loss_content: 1.79713e+08, Loss_style: 59152.7\n",
            "iteration:  844,  Loss_total: 238708, Loss_content: 1.7971e+08, Loss_style: 58998.6\n",
            "iteration:  845,  Loss_total: 238573, Loss_content: 1.79704e+08, Loss_style: 58869.5\n",
            "iteration:  846,  Loss_total: 238408, Loss_content: 1.79656e+08, Loss_style: 58752.3\n",
            "iteration:  847,  Loss_total: 238253, Loss_content: 1.79633e+08, Loss_style: 58620.1\n",
            "iteration:  848,  Loss_total: 238107, Loss_content: 1.79598e+08, Loss_style: 58508.5\n",
            "iteration:  849,  Loss_total: 237936, Loss_content: 1.79557e+08, Loss_style: 58378.9\n",
            "iteration:  850,  Loss_total: 237802, Loss_content: 1.79471e+08, Loss_style: 58331.1\n",
            "iteration:  851,  Loss_total: 237594, Loss_content: 1.79475e+08, Loss_style: 58119\n",
            "iteration:  852,  Loss_total: 237449, Loss_content: 1.79466e+08, Loss_style: 57983.1\n",
            "iteration:  853,  Loss_total: 237271, Loss_content: 1.79446e+08, Loss_style: 57825.7\n",
            "iteration:  854,  Loss_total: 237108, Loss_content: 1.79372e+08, Loss_style: 57736.4\n",
            "iteration:  855,  Loss_total: 236836, Loss_content: 1.79347e+08, Loss_style: 57488.8\n",
            "iteration:  856,  Loss_total: 236683, Loss_content: 1.7932e+08, Loss_style: 57363.1\n",
            "iteration:  857,  Loss_total: 236500, Loss_content: 1.79249e+08, Loss_style: 57251.9\n",
            "iteration:  858,  Loss_total: 236355, Loss_content: 1.79226e+08, Loss_style: 57128.9\n",
            "iteration:  859,  Loss_total: 236240, Loss_content: 1.79221e+08, Loss_style: 57019.2\n",
            "iteration:  860,  Loss_total: 236042, Loss_content: 1.79179e+08, Loss_style: 56863\n",
            "iteration:  861,  Loss_total: 235901, Loss_content: 1.79153e+08, Loss_style: 56747.3\n",
            "iteration:  862,  Loss_total: 235695, Loss_content: 1.79106e+08, Loss_style: 56588.4\n",
            "iteration:  863,  Loss_total: 235517, Loss_content: 1.79049e+08, Loss_style: 56468.5\n",
            "iteration:  864,  Loss_total: 235401, Loss_content: 1.79032e+08, Loss_style: 56369\n",
            "iteration:  865,  Loss_total: 235264, Loss_content: 1.79019e+08, Loss_style: 56244.8\n",
            "iteration:  866,  Loss_total: 235041, Loss_content: 1.78992e+08, Loss_style: 56049\n",
            "iteration:  867,  Loss_total: 234914, Loss_content: 1.78978e+08, Loss_style: 55936.2\n",
            "iteration:  868,  Loss_total: 234816, Loss_content: 1.78874e+08, Loss_style: 55942.2\n",
            "iteration:  869,  Loss_total: 234382, Loss_content: 1.78868e+08, Loss_style: 55514\n",
            "iteration:  870,  Loss_total: 234265, Loss_content: 1.78849e+08, Loss_style: 55416.4\n",
            "iteration:  871,  Loss_total: 234126, Loss_content: 1.78798e+08, Loss_style: 55328\n",
            "iteration:  872,  Loss_total: 233982, Loss_content: 1.78761e+08, Loss_style: 55220.5\n",
            "iteration:  873,  Loss_total: 233819, Loss_content: 1.78711e+08, Loss_style: 55108.1\n",
            "iteration:  874,  Loss_total: 233665, Loss_content: 1.7863e+08, Loss_style: 55035\n",
            "iteration:  875,  Loss_total: 233398, Loss_content: 1.78585e+08, Loss_style: 54812.5\n",
            "iteration:  876,  Loss_total: 233285, Loss_content: 1.78597e+08, Loss_style: 54688.8\n",
            "iteration:  877,  Loss_total: 233177, Loss_content: 1.78578e+08, Loss_style: 54598.6\n",
            "iteration:  878,  Loss_total: 233031, Loss_content: 1.78526e+08, Loss_style: 54504.8\n",
            "iteration:  879,  Loss_total: 232856, Loss_content: 1.78485e+08, Loss_style: 54371.2\n",
            "iteration:  880,  Loss_total: 232604, Loss_content: 1.784e+08, Loss_style: 54204.4\n",
            "iteration:  881,  Loss_total: 232415, Loss_content: 1.78348e+08, Loss_style: 54066.7\n",
            "iteration:  882,  Loss_total: 232278, Loss_content: 1.78339e+08, Loss_style: 53939.2\n",
            "iteration:  883,  Loss_total: 232107, Loss_content: 1.78343e+08, Loss_style: 53763.7\n",
            "iteration:  884,  Loss_total: 231944, Loss_content: 1.78308e+08, Loss_style: 53636.8\n",
            "iteration:  885,  Loss_total: 231819, Loss_content: 1.78318e+08, Loss_style: 53500.2\n",
            "iteration:  886,  Loss_total: 231639, Loss_content: 1.78284e+08, Loss_style: 53354.6\n",
            "iteration:  887,  Loss_total: 231526, Loss_content: 1.78242e+08, Loss_style: 53284\n",
            "iteration:  888,  Loss_total: 231384, Loss_content: 1.78205e+08, Loss_style: 53178.4\n",
            "iteration:  889,  Loss_total: 231179, Loss_content: 1.78119e+08, Loss_style: 53059.5\n",
            "iteration:  890,  Loss_total: 230991, Loss_content: 1.78108e+08, Loss_style: 52883.3\n",
            "iteration:  891,  Loss_total: 230867, Loss_content: 1.78095e+08, Loss_style: 52771.8\n",
            "iteration:  892,  Loss_total: 230626, Loss_content: 1.78046e+08, Loss_style: 52580.3\n",
            "iteration:  893,  Loss_total: 230561, Loss_content: 1.77985e+08, Loss_style: 52576\n",
            "iteration:  894,  Loss_total: 230320, Loss_content: 1.77966e+08, Loss_style: 52353.9\n",
            "iteration:  895,  Loss_total: 230220, Loss_content: 1.77972e+08, Loss_style: 52248.1\n",
            "iteration:  896,  Loss_total: 230101, Loss_content: 1.77941e+08, Loss_style: 52159.7\n",
            "iteration:  897,  Loss_total: 229877, Loss_content: 1.7784e+08, Loss_style: 52037.2\n",
            "iteration:  898,  Loss_total: 229677, Loss_content: 1.77807e+08, Loss_style: 51870.4\n",
            "iteration:  899,  Loss_total: 229538, Loss_content: 1.77791e+08, Loss_style: 51746.8\n",
            "iteration:  900,  Loss_total: 229438, Loss_content: 1.77758e+08, Loss_style: 51680.3\n",
            "iteration:  901,  Loss_total: 229286, Loss_content: 1.77742e+08, Loss_style: 51543.4\n",
            "iteration:  902,  Loss_total: 229044, Loss_content: 1.77657e+08, Loss_style: 51386.6\n",
            "iteration:  903,  Loss_total: 228866, Loss_content: 1.77638e+08, Loss_style: 51227.7\n",
            "iteration:  904,  Loss_total: 228721, Loss_content: 1.77603e+08, Loss_style: 51118.5\n",
            "iteration:  905,  Loss_total: 228595, Loss_content: 1.77567e+08, Loss_style: 51028.5\n",
            "iteration:  906,  Loss_total: 228421, Loss_content: 1.77514e+08, Loss_style: 50906.8\n",
            "iteration:  907,  Loss_total: 228252, Loss_content: 1.77477e+08, Loss_style: 50774.6\n",
            "iteration:  908,  Loss_total: 228188, Loss_content: 1.77417e+08, Loss_style: 50770.9\n",
            "iteration:  909,  Loss_total: 227972, Loss_content: 1.77427e+08, Loss_style: 50544.6\n",
            "iteration:  910,  Loss_total: 227843, Loss_content: 1.77402e+08, Loss_style: 50440.3\n",
            "iteration:  911,  Loss_total: 227679, Loss_content: 1.77383e+08, Loss_style: 50296.4\n",
            "iteration:  912,  Loss_total: 227508, Loss_content: 1.77319e+08, Loss_style: 50189.1\n",
            "iteration:  913,  Loss_total: 227349, Loss_content: 1.7729e+08, Loss_style: 50059.1\n",
            "iteration:  914,  Loss_total: 227211, Loss_content: 1.77279e+08, Loss_style: 49931.3\n",
            "iteration:  915,  Loss_total: 227073, Loss_content: 1.77223e+08, Loss_style: 49850.1\n",
            "iteration:  916,  Loss_total: 226941, Loss_content: 1.7721e+08, Loss_style: 49731.6\n",
            "iteration:  917,  Loss_total: 226840, Loss_content: 1.77208e+08, Loss_style: 49632.1\n",
            "iteration:  918,  Loss_total: 226584, Loss_content: 1.77153e+08, Loss_style: 49430.1\n",
            "iteration:  919,  Loss_total: 226464, Loss_content: 1.77118e+08, Loss_style: 49345.9\n",
            "iteration:  920,  Loss_total: 226310, Loss_content: 1.77099e+08, Loss_style: 49210.3\n",
            "iteration:  921,  Loss_total: 226214, Loss_content: 1.77067e+08, Loss_style: 49147.2\n",
            "iteration:  922,  Loss_total: 226095, Loss_content: 1.7704e+08, Loss_style: 49054.7\n",
            "iteration:  923,  Loss_total: 225954, Loss_content: 1.76987e+08, Loss_style: 48967.4\n",
            "iteration:  924,  Loss_total: 225783, Loss_content: 1.76942e+08, Loss_style: 48841.5\n",
            "iteration:  925,  Loss_total: 225584, Loss_content: 1.76892e+08, Loss_style: 48692.4\n",
            "iteration:  926,  Loss_total: 225475, Loss_content: 1.76801e+08, Loss_style: 48673.2\n",
            "iteration:  927,  Loss_total: 225286, Loss_content: 1.7681e+08, Loss_style: 48476\n",
            "iteration:  928,  Loss_total: 225166, Loss_content: 1.76796e+08, Loss_style: 48370.4\n",
            "iteration:  929,  Loss_total: 225040, Loss_content: 1.76763e+08, Loss_style: 48277\n",
            "iteration:  930,  Loss_total: 224881, Loss_content: 1.76702e+08, Loss_style: 48179.6\n",
            "iteration:  931,  Loss_total: 224732, Loss_content: 1.76646e+08, Loss_style: 48085.7\n",
            "iteration:  932,  Loss_total: 224576, Loss_content: 1.76634e+08, Loss_style: 47942\n",
            "iteration:  933,  Loss_total: 224465, Loss_content: 1.76612e+08, Loss_style: 47853.4\n",
            "iteration:  934,  Loss_total: 224381, Loss_content: 1.76593e+08, Loss_style: 47787.6\n",
            "iteration:  935,  Loss_total: 224219, Loss_content: 1.76552e+08, Loss_style: 47666.6\n",
            "iteration:  936,  Loss_total: 224072, Loss_content: 1.76483e+08, Loss_style: 47589.5\n",
            "iteration:  937,  Loss_total: 223985, Loss_content: 1.765e+08, Loss_style: 47485.9\n",
            "iteration:  938,  Loss_total: 223870, Loss_content: 1.76491e+08, Loss_style: 47379.5\n",
            "iteration:  939,  Loss_total: 223759, Loss_content: 1.76463e+08, Loss_style: 47296.9\n",
            "iteration:  940,  Loss_total: 223642, Loss_content: 1.76439e+08, Loss_style: 47202.9\n",
            "iteration:  941,  Loss_total: 223528, Loss_content: 1.76383e+08, Loss_style: 47144.6\n",
            "iteration:  942,  Loss_total: 223298, Loss_content: 1.76351e+08, Loss_style: 46946.5\n",
            "iteration:  943,  Loss_total: 223201, Loss_content: 1.76345e+08, Loss_style: 46855.7\n",
            "iteration:  944,  Loss_total: 223076, Loss_content: 1.763e+08, Loss_style: 46776.4\n",
            "iteration:  945,  Loss_total: 223124, Loss_content: 1.76249e+08, Loss_style: 46875.2\n",
            "iteration:  946,  Loss_total: 223001, Loss_content: 1.76277e+08, Loss_style: 46724.1\n",
            "iteration:  947,  Loss_total: 222872, Loss_content: 1.76248e+08, Loss_style: 46623.5\n",
            "iteration:  948,  Loss_total: 222743, Loss_content: 1.76201e+08, Loss_style: 46542.4\n",
            "iteration:  949,  Loss_total: 222655, Loss_content: 1.76178e+08, Loss_style: 46477.1\n",
            "iteration:  950,  Loss_total: 222543, Loss_content: 1.76154e+08, Loss_style: 46389.4\n",
            "iteration:  951,  Loss_total: 222397, Loss_content: 1.76113e+08, Loss_style: 46284.3\n",
            "iteration:  952,  Loss_total: 222280, Loss_content: 1.7608e+08, Loss_style: 46200.5\n",
            "iteration:  953,  Loss_total: 222195, Loss_content: 1.76004e+08, Loss_style: 46190.9\n",
            "iteration:  954,  Loss_total: 222014, Loss_content: 1.76e+08, Loss_style: 46013.5\n",
            "iteration:  955,  Loss_total: 221927, Loss_content: 1.75982e+08, Loss_style: 45944.5\n",
            "iteration:  956,  Loss_total: 221817, Loss_content: 1.7595e+08, Loss_style: 45867.3\n",
            "iteration:  957,  Loss_total: 221662, Loss_content: 1.7592e+08, Loss_style: 45742.9\n",
            "iteration:  958,  Loss_total: 221527, Loss_content: 1.75851e+08, Loss_style: 45676.3\n",
            "iteration:  959,  Loss_total: 221390, Loss_content: 1.75849e+08, Loss_style: 45541\n",
            "iteration:  960,  Loss_total: 221309, Loss_content: 1.75838e+08, Loss_style: 45471.1\n",
            "iteration:  961,  Loss_total: 221149, Loss_content: 1.75814e+08, Loss_style: 45334.2\n",
            "iteration:  962,  Loss_total: 221222, Loss_content: 1.75701e+08, Loss_style: 45521.6\n",
            "iteration:  963,  Loss_total: 221034, Loss_content: 1.75762e+08, Loss_style: 45272.4\n",
            "iteration:  964,  Loss_total: 220873, Loss_content: 1.75736e+08, Loss_style: 45136.3\n",
            "iteration:  965,  Loss_total: 220789, Loss_content: 1.75683e+08, Loss_style: 45106.4\n",
            "iteration:  966,  Loss_total: 220675, Loss_content: 1.75689e+08, Loss_style: 44986.3\n",
            "iteration:  967,  Loss_total: 220592, Loss_content: 1.75678e+08, Loss_style: 44914.1\n",
            "iteration:  968,  Loss_total: 220436, Loss_content: 1.75636e+08, Loss_style: 44800.8\n",
            "iteration:  969,  Loss_total: 220325, Loss_content: 1.75577e+08, Loss_style: 44747.9\n",
            "iteration:  970,  Loss_total: 220168, Loss_content: 1.75558e+08, Loss_style: 44610.1\n",
            "iteration:  971,  Loss_total: 220053, Loss_content: 1.75522e+08, Loss_style: 44530.4\n",
            "iteration:  972,  Loss_total: 219963, Loss_content: 1.75505e+08, Loss_style: 44458.6\n",
            "iteration:  973,  Loss_total: 219835, Loss_content: 1.7545e+08, Loss_style: 44385.6\n",
            "iteration:  974,  Loss_total: 219682, Loss_content: 1.75444e+08, Loss_style: 44238\n",
            "iteration:  975,  Loss_total: 219566, Loss_content: 1.75425e+08, Loss_style: 44140.8\n",
            "iteration:  976,  Loss_total: 219424, Loss_content: 1.75402e+08, Loss_style: 44021.8\n",
            "iteration:  977,  Loss_total: 219333, Loss_content: 1.75337e+08, Loss_style: 43996.1\n",
            "iteration:  978,  Loss_total: 219210, Loss_content: 1.75325e+08, Loss_style: 43884.7\n",
            "iteration:  979,  Loss_total: 219124, Loss_content: 1.75333e+08, Loss_style: 43790.2\n",
            "iteration:  980,  Loss_total: 219062, Loss_content: 1.75309e+08, Loss_style: 43752.5\n",
            "iteration:  981,  Loss_total: 218870, Loss_content: 1.75248e+08, Loss_style: 43622.1\n",
            "iteration:  982,  Loss_total: 218834, Loss_content: 1.75156e+08, Loss_style: 43678.1\n",
            "iteration:  983,  Loss_total: 218654, Loss_content: 1.75183e+08, Loss_style: 43470.7\n",
            "iteration:  984,  Loss_total: 218590, Loss_content: 1.75179e+08, Loss_style: 43411.6\n",
            "iteration:  985,  Loss_total: 218475, Loss_content: 1.75164e+08, Loss_style: 43311.1\n",
            "iteration:  986,  Loss_total: 218313, Loss_content: 1.7512e+08, Loss_style: 43192.6\n",
            "iteration:  987,  Loss_total: 218223, Loss_content: 1.7504e+08, Loss_style: 43183\n",
            "iteration:  988,  Loss_total: 218001, Loss_content: 1.75029e+08, Loss_style: 42971.7\n",
            "iteration:  989,  Loss_total: 217939, Loss_content: 1.75027e+08, Loss_style: 42912.1\n",
            "iteration:  990,  Loss_total: 217833, Loss_content: 1.75009e+08, Loss_style: 42824.9\n",
            "iteration:  991,  Loss_total: 217871, Loss_content: 1.7495e+08, Loss_style: 42920.9\n",
            "iteration:  992,  Loss_total: 217753, Loss_content: 1.74982e+08, Loss_style: 42770.8\n",
            "iteration:  993,  Loss_total: 217642, Loss_content: 1.74969e+08, Loss_style: 42673.3\n",
            "iteration:  994,  Loss_total: 217513, Loss_content: 1.74936e+08, Loss_style: 42576.7\n",
            "iteration:  995,  Loss_total: 217440, Loss_content: 1.74928e+08, Loss_style: 42512\n",
            "iteration:  996,  Loss_total: 217349, Loss_content: 1.7491e+08, Loss_style: 42438.7\n",
            "iteration:  997,  Loss_total: 217104, Loss_content: 1.74794e+08, Loss_style: 42310.6\n",
            "iteration:  998,  Loss_total: 217153, Loss_content: 1.74743e+08, Loss_style: 42409.3\n",
            "iteration:  999,  Loss_total: 216971, Loss_content: 1.7477e+08, Loss_style: 42201\n",
            "iteration: 1000,  Loss_total: 216883, Loss_content: 1.74759e+08, Loss_style: 42124.2\n",
            "iteration: 1001,  Loss_total: 216782, Loss_content: 1.74722e+08, Loss_style: 42060.2\n",
            "iteration: 1002,  Loss_total: 216662, Loss_content: 1.7469e+08, Loss_style: 41971.7\n",
            "iteration: 1003,  Loss_total: 216560, Loss_content: 1.74598e+08, Loss_style: 41961.6\n",
            "iteration: 1004,  Loss_total: 216400, Loss_content: 1.74602e+08, Loss_style: 41798.6\n",
            "iteration: 1005,  Loss_total: 216332, Loss_content: 1.74597e+08, Loss_style: 41735.1\n",
            "iteration: 1006,  Loss_total: 216236, Loss_content: 1.74581e+08, Loss_style: 41655\n",
            "iteration: 1007,  Loss_total: 216124, Loss_content: 1.74531e+08, Loss_style: 41592.6\n",
            "iteration: 1008,  Loss_total: 216004, Loss_content: 1.74506e+08, Loss_style: 41498.7\n",
            "iteration: 1009,  Loss_total: 215886, Loss_content: 1.74465e+08, Loss_style: 41420.3\n",
            "iteration: 1010,  Loss_total: 215803, Loss_content: 1.74444e+08, Loss_style: 41359.4\n",
            "iteration: 1011,  Loss_total: 215687, Loss_content: 1.74397e+08, Loss_style: 41290.2\n",
            "iteration: 1012,  Loss_total: 215573, Loss_content: 1.74345e+08, Loss_style: 41228.3\n",
            "iteration: 1013,  Loss_total: 215464, Loss_content: 1.74334e+08, Loss_style: 41130.7\n",
            "iteration: 1014,  Loss_total: 215386, Loss_content: 1.74324e+08, Loss_style: 41062.2\n",
            "iteration: 1015,  Loss_total: 215244, Loss_content: 1.743e+08, Loss_style: 40943.7\n",
            "iteration: 1016,  Loss_total: 215124, Loss_content: 1.74237e+08, Loss_style: 40887.2\n",
            "iteration: 1017,  Loss_total: 214930, Loss_content: 1.74217e+08, Loss_style: 40712.3\n",
            "iteration: 1018,  Loss_total: 214811, Loss_content: 1.74188e+08, Loss_style: 40622.1\n",
            "iteration: 1019,  Loss_total: 214692, Loss_content: 1.74158e+08, Loss_style: 40534.2\n",
            "iteration: 1020,  Loss_total: 214658, Loss_content: 1.74076e+08, Loss_style: 40581.7\n",
            "iteration: 1021,  Loss_total: 214621, Loss_content: 1.74113e+08, Loss_style: 40508.1\n",
            "iteration: 1022,  Loss_total: 214488, Loss_content: 1.7407e+08, Loss_style: 40418\n",
            "iteration: 1023,  Loss_total: 214352, Loss_content: 1.74013e+08, Loss_style: 40339.5\n",
            "iteration: 1024,  Loss_total: 214266, Loss_content: 1.73998e+08, Loss_style: 40268.8\n",
            "iteration: 1025,  Loss_total: 214185, Loss_content: 1.73985e+08, Loss_style: 40200.1\n",
            "iteration: 1026,  Loss_total: 214050, Loss_content: 1.73933e+08, Loss_style: 40116.4\n",
            "iteration: 1027,  Loss_total: 213916, Loss_content: 1.73909e+08, Loss_style: 40006.7\n",
            "iteration: 1028,  Loss_total: 213826, Loss_content: 1.73897e+08, Loss_style: 39928.4\n",
            "iteration: 1029,  Loss_total: 213740, Loss_content: 1.73851e+08, Loss_style: 39889.4\n",
            "iteration: 1030,  Loss_total: 213673, Loss_content: 1.73853e+08, Loss_style: 39819.8\n",
            "iteration: 1031,  Loss_total: 213593, Loss_content: 1.73841e+08, Loss_style: 39751.4\n",
            "iteration: 1032,  Loss_total: 213447, Loss_content: 1.738e+08, Loss_style: 39647.2\n",
            "iteration: 1033,  Loss_total: 213338, Loss_content: 1.73772e+08, Loss_style: 39566.2\n",
            "iteration: 1034,  Loss_total: 213201, Loss_content: 1.73716e+08, Loss_style: 39485.2\n",
            "iteration: 1035,  Loss_total: 213064, Loss_content: 1.73689e+08, Loss_style: 39374.9\n",
            "iteration: 1036,  Loss_total: 212997, Loss_content: 1.73704e+08, Loss_style: 39293.3\n",
            "INFO:tensorflow:Optimization terminated with:\n",
            "  Message: b'STOP: TOTAL NO. of ITERATIONS REACHED LIMIT'\n",
            "  Objective function value: 212997.281250\n",
            "  Number of iterations: 1000\n",
            "  Number of functions evaluations: 1037\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uCqaN5jdgP2K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7024fce8-933a-41a0-ae28-f593560d534d"
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mixed_portait.jpg  picture.jpeg  portrait.jpg  pre_trained_model  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pMTQZjEqg43V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "files.download(args_output)  # download the mixed_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NV_RF-Qlpr0Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}